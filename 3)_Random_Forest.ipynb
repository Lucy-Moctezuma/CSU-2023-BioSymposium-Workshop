{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lucy-Moctezuma/ML-Tutorial-for-Antibiotic-Resistance-Predictions-for-E.-Coli/blob/main/3)_Random_Forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(**Note:**\n",
        "Click on the button that reads *“Open in Colab”* to open this code in Google Colab. Once open in Google Colab, you can make a copy of the notebook in your personal drive and run the code by clicking a little triangle/arrow to the left of each code block.)"
      ],
      "metadata": {
        "id": "OpSc3R9k3aTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random Forest**\n",
        "## ***Objectives for this Notebook***\n",
        "- Familiarize with the basics of how tree models works, specifically Random Forests.\n",
        "- Implementing functions to run Random Forest to Moragadivand's dataset.\n",
        "\n",
        "<a name=\"tree-methods\"></a>\n",
        "Random Forests belong to a group of algorithms known as tree methods. A Random Forest model is also an ensemble method. Ensemble methods use an arrangement of weaker predictor models (in this case Decision Trees) to produce a stronger model (in this case Random Forest). The basis of all tree methods is the Decision Tree. For those unfamiliar with tree methods, we have some definitions below:\n",
        "\n",
        "![tree_forest.png](https://drive.google.com/uc?export=view&id=1TBxIoL7K4WeXhegZZAvURJSxREw-Uqto)\n",
        "\n",
        "- A **Decision Tree (left)** is a supervised learning algorithm that can be used for regression (predicting a continuous variable) or for classification (predicting a categorical variable). Our task is to classify each *E. coli* sample into Resistant (**R**) or Susceptible (**S**) classes.\n",
        "We can picture a tree upside down, starting with the root on top and stopping at the leafs at the bottom, the different squares represent decision thresholds, if the isolate passes the threshold, then we move down the tree following the \"yes\" arrow, if not then we follow the \"no\" arrow to the next square down below until a classification decision is finally made.\n",
        "\n",
        "- A **Random Forest (right)** is the combination of several Decision Trees outputs, each tree classifies all samples independently and the final decision for each sample is made based on the decision of the majority of trees. Each tree inside the Random Forest is created by selecting a random subset of features instead of using all the features.\n",
        "\n",
        "- **Bagging:** To make a prediction with a Random Forest model, we average or agregate the results from all the different Decision Trees we have created for a final prediction. This process is called Bagging. Bagging is useful because it reduces overfitting and variance.\n",
        "  \n",
        "If we chose to use a combination of features such as: **GY** (Gene Presence or Absence + Year of isolation), each tree would randomly use only some columns from the total amount of column features (G + Y features).\n",
        "\n",
        "We will explain further details on the structure of Random Forests as we code along to further clarify how nodes are chosen and how it uses the outputs of all trees for a final decision.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4s_KUXfGSthq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1) Importing Packages needed**\n",
        "\n",
        "**NOTE:** Similar code as in previous notebook"
      ],
      "metadata": {
        "id": "fnpxHzYUUJ_M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDa820s0JSoJ"
      },
      "outputs": [],
      "source": [
        "# Data manipulation imports for ML\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas.api.types import is_numeric_dtype\n",
        "\n",
        "# Import packages for Random Forest model and hyperparameter tuning\n",
        "from sklearn.ensemble  import RandomForestClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.model_selection import StratifiedGroupKFold, KFold\n",
        "\n",
        "# Imports for model evaluation\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, get_scorer_names\n",
        "from sklearn.metrics import f1_score, make_scorer, accuracy_score, recall_score, precision_score\n",
        "import warnings\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "warnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)\n",
        "\n",
        "# Imports for data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot\n",
        "import seaborn as sns\n",
        "import graphviz\n",
        "\n",
        "# Imports for file management\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2) Loading CSV file and creating dataframes for each antibiotic**\n",
        "\n",
        "Similar to the previous notebook we loaded the dataframe created in the first notebook of this tutorial, then we create a dataframe for each antibiotic. To check that our function works we will test it using \"CTX\" antibiotic.\n",
        "\n",
        "**NOTE:** Same code as in previous notebook"
      ],
      "metadata": {
        "id": "1MTk8vozVJqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads csv file as a dataframe\n",
        "filepath = '/content/drive/My Drive/EColi_ML_CSV_files/'\n",
        "\n",
        "# reads csv file as a dataframe\n",
        "All_Drugs_df = pd.read_csv(filepath+\"EColi_Merged_df.csv\", na_values=\"NaN\")\n",
        "All_Drugs_df.head()"
      ],
      "metadata": {
        "id": "psqfFp_yVsmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3) Separating each Drug Dataframe into 4 sections : Training (features and labels) and Testing (features and labels)**\n",
        "\n",
        "**NOTE:** Same code as in previous notebook"
      ],
      "metadata": {
        "id": "NMKzeaqPojso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating each dataframe into Labels and Features for training and testing data.\n",
        "# Our function uses the handy train_test_split() function.\n",
        "\n",
        "def Split_train_test_antibiotic(drug):\n",
        "  #here we make a list of the columns we want to keep: the column for the isolate, the column for the drug we are interested in and all features (starting from column 15).\n",
        "  df_list = [All_Drugs_df[[\"MLST\",\"Isolate\",drug,\"Year\"]], All_Drugs_df.iloc[:,15:]]\n",
        "\n",
        "  #here we create a data frame with just the columns we wanted to keep.\n",
        "  Drug_df = pd.concat(df_list, axis=1)\n",
        "\n",
        "  #here we drop all rows with missing data\n",
        "  Drug_df = Drug_df.dropna()\n",
        "\n",
        "  # Creating a dictionary to store each antibiotic datasets\n",
        "  Train_test_dic = {}\n",
        "\n",
        "  # Defining the label columns\n",
        "  labels = Drug_df[drug]\n",
        "\n",
        "  # Defining features columns\n",
        "  features = Drug_df.drop(columns=[drug])\n",
        "\n",
        "  # Separating training (features and labels) and testing (features and labels) datasets\n",
        "  features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.20, random_state=42, stratify=labels)\n",
        "\n",
        "  # storing each data chunk in a dictionary\n",
        "  Train_test_dic['labels_train'] = labels_train\n",
        "  Train_test_dic['features_train'] = features_train\n",
        "  Train_test_dic['labels_test'] = labels_test\n",
        "  Train_test_dic['features_test'] = features_test\n",
        "\n",
        "  return Train_test_dic"
      ],
      "metadata": {
        "id": "iP4HEW4qBO7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we implement the function **Split_train_test_antibiotic()** and then print how many are resistant and how many are Susceptible for the antibiotic CTX."
      ],
      "metadata": {
        "id": "_fAor5uHzNQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the function Split_train_test_antibiotic() for CTX example\n",
        "CTX_Train_test_dic = Split_train_test_antibiotic(\"CTX\")\n",
        "\n",
        "# checking the shape of each dataframe or series stored in the dictionary created for drug CTX\n",
        "print(\"CTX\")\n",
        "for k, df in CTX_Train_test_dic.items():\n",
        "  print(k, df.shape)\n",
        "  # counting how many of the labels have susceptible versus resistant ones\n",
        "  if k.startswith(\"label\"):\n",
        "    print(df.value_counts())"
      ],
      "metadata": {
        "id": "rLhddtjJt9Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that just like before we have more *E. coli* samples that are Susceptible, rather than Resistant, therefore we need to remember to pay attention to the metrics of precision and recall for each of these classes rather than just looking at the overall accuracy. **The total number of training observations is 1440.**"
      ],
      "metadata": {
        "id": "YWUZNSEn-ZEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4) Creating different combination of features before training**\n",
        "\n",
        "**NOTE:** Same code as in previous notebook"
      ],
      "metadata": {
        "id": "2IyqN506dQn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making a list of combinations of data sources we would like to test in our ML models\n",
        "combo_list = ['Y', 'G', 'GY']\n",
        "\n",
        "# making a function that creates different feature combinations of the predictor features\n",
        "def combo_feat(features_df, drug, combo):\n",
        "\n",
        "  # Isolating Year as a feature and\n",
        "  year_filter = [col for col in features_df if col.startswith(\"Year\")]\n",
        "  year_feat = features_df[year_filter]\n",
        "\n",
        "  # creating Gene precence column filters for features_df\n",
        "  gene_presc_filter = [col for col in features_df.columns if col not in year_filter and col != \"Isolate\"]\n",
        "  gene_presc_feat = features_df[gene_presc_filter]\n",
        "\n",
        "  if combo == 'Y':\n",
        "    df_list = [features_df[['MLST','Isolate']], year_feat]\n",
        "    Y_feat_df = pd.concat(df_list, axis=1)\n",
        "    Y_feat_df = Y_feat_df.drop(columns=['Isolate'])\n",
        "    return Y_feat_df\n",
        "\n",
        "  if combo == 'G':\n",
        "    df_list = [features_df['Isolate'], gene_presc_feat]\n",
        "    G_feat_df = pd.concat(df_list, axis=1)\n",
        "    G_feat_df = G_feat_df.drop(columns=['Isolate'])\n",
        "    return G_feat_df\n",
        "\n",
        "  if combo == 'GY':\n",
        "    df_list = [features_df['Isolate'], year_feat, gene_presc_feat]\n",
        "    GY_feat_df = pd.concat(df_list, axis=1)\n",
        "    GY_feat_df = GY_feat_df.drop(columns=['Isolate'])\n",
        "    return GY_feat_df"
      ],
      "metadata": {
        "id": "JmzbwQBxlylk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing combo_feat() function created for training data\n",
        "CTX_G_train_df = combo_feat(CTX_Train_test_dic['features_train'],\"CTX\",\"G\")\n",
        "\n",
        "# looking only at the feature column names for the combination for \"G\" for drug \"CTX\" for training data\n",
        "CTX_G_train_df.columns"
      ],
      "metadata": {
        "id": "ERB_2YEXuAsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5) Creating Random Forest model and training it per feature combination**\n",
        "In this step we can take a closer look at the structure of a Random Forest. First we will create a function that will train our Random Forest to learn to predict Resistance or Susceptibility for a particular strain. Because our function does not specify how many trees to use, the default is 100 Decision Trees created.\n",
        "\n",
        "- We have set a random state in our in the Random Forest Classifier in order to get replicable results for this tutorial."
      ],
      "metadata": {
        "id": "ZVOKCEZNeHzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating Random Forest model function\n",
        "def run_RF(feat_train, lab_train, drug, combo):\n",
        "  print(drug +\" Training combo: \"+ combo)\n",
        "  feat = feat_train.drop(columns=[\"MLST\"])\n",
        "  # creating Random Forest model\n",
        "  RF = RandomForestClassifier(random_state = 42)\n",
        "  # Training Random Forest Model\n",
        "  RF = RF.fit(feat, lab_train)\n",
        "  # Checking number of trees in the model\n",
        "  print(\"Number of Decicion Trees in RF model:\", len(RF))\n",
        "  return RF"
      ],
      "metadata": {
        "id": "QbEP2WYkWPxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we will implement our function just as in previous parts, but then we will also take a look at one tree of the Random Forest, after training."
      ],
      "metadata": {
        "id": "58ZumF78K_cW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# implementing run_RF() for specific drug feature combination dataframe\n",
        "RF_CTX_G_model = run_RF(CTX_G_train_df,CTX_Train_test_dic[\"labels_train\"],\"CTX\",\"G\")\n",
        "RF_CTX_G_model"
      ],
      "metadata": {
        "id": "jxDDUGtfuCp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have trained our Random Forest model using only Gene Absence / Presence (G) features to predict resistance to the antibiotic CTX. Below we can graph for example the fourth tree (index=3). We have 100 trees, and python starts count from 0. Therefore the index would runs from 0 to 99."
      ],
      "metadata": {
        "id": "ZbNGU0btNxoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the index of the tree we are interested in seeing\n",
        "index= 3\n",
        "\n",
        "# Drawing the tree by setting the Index and the depth of the tree we would like to draw\n",
        "single_tree = tree.export_graphviz(RF_CTX_G_model.estimators_[index], out_file=None,\n",
        "                                  feature_names=CTX_G_train_df.iloc[:,1:17199].columns,\n",
        "                                  filled=True, rounded=True,\n",
        "                                  special_characters=True,\n",
        "                                  max_depth=None)\n",
        "graph = graphviz.Source(single_tree)"
      ],
      "metadata": {
        "id": "wihNsmd3Okiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below will store our complete Decision Tree graph as a pdf document. The file will be named based on the index you have chosen to draw, in this case it would be the tree at index 3. Notice that a single tree from our Random Forest can be very big. Make sure to look at the file that will be saved to a folder called \"EColi_ML_Plots\" in your Drive."
      ],
      "metadata": {
        "id": "GAip5wRdkTPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving a single complete decision tree from our Random Forest model into our new directory\n",
        "graph.render(\"/content/drive/MyDrive/EColi_ML_Plots/RF_Classification Tree #: \"+ str(index))"
      ],
      "metadata": {
        "id": "5ZE6BsYLj2wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below on the other hand will let us view only the top part of the tree (only first 3 layers after the root node), you can control the display of the tree by changing the argument **max_depth**.\n",
        "You can see here that the first node looks at whether an isolate carries the gene referred to as group_6670. The second node on the left looks at whether isolates carry a gene that is referred to as \"hsdR\"."
      ],
      "metadata": {
        "id": "iqYocJWXhr4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Parts of a Decision Tree**:\n",
        "\n",
        "- **Root Node:** displayed at the very top of our Decision tree. It is considered as the most important feature from the random subset of feature columns that our Random Forest (RF) picked. Therefore, each tree in the RF is likely to have a different Root Node.\n",
        "\n",
        "- **Intermediate Nodes:** the first 2 layers of intermediate nodes are being displayed. Similar to the Root Node it contains decision thresholds that eventually lead to a final prediction.\n",
        "\n",
        "- **Leaf Nodes:** Not displayed in the trimmed version, but we can observe them in the pdf. The leaf nodes contain the final predictions within one tree of our RF. You can recognize them because there are no decision thresholds."
      ],
      "metadata": {
        "id": "wDdjIssHyVgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the index of the tree we are interested in seeing and how many layers to draw\n",
        "index= 3\n",
        "\n",
        "# Drawing the tree by setting the Index and the depth of the tree we would like to draw\n",
        "chopped_tree = tree.export_graphviz(RF_CTX_G_model.estimators_[index], out_file=None,\n",
        "                                  feature_names=CTX_G_train_df.iloc[:,1:17199].columns,\n",
        "                                  filled=False, rounded=True,\n",
        "                                  special_characters=True,\n",
        "                                  max_depth=2)\n",
        "graph = graphviz.Source(chopped_tree)\n",
        "display(graph)"
      ],
      "metadata": {
        "id": "xgStmSKyjQUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Information within each Node**\n",
        "\n",
        "Below is an schematic for each type of node and uses the fourth Decision Tree of our Random Forest. The type of information displayed in each node, has been labeled (A, B, C and D) only for the root node but it is present in all the root and intermediate nodes, the leaf nodes only contain B, C and D information. Below we will go over what each of the labeled information represent:\n",
        "\n",
        "![tree-parts.png](https://drive.google.com/uc?export=view&id=1WN05o44tqflVq-GL_2uL_nQuLn3MiNEG)\n",
        "***\n",
        "**A) Decision Threshold:** These are essentially Boolean statement about a column feature that represents a threshold. If the statement is TRUE for a particular observation then we follow the left arrow to and if its false FALSE then we follow the right arrow to the next node. The Decision Threshold is only present in the root and intermediate nodes. In the picture example the thresholds would be: <font face=\"monospace\">group_6670 $\\leq$ 0.5 (root node), hsdR_1 $\\leq$ 0.5 (intermediate node), aacA-aphD $\\leq$ 0.5\n",
        " (intermediate node)</font>, etc\n",
        "\n",
        "The features from the gene presence absense table (gene names or group numbers) are categorical variables that have already been one-hot encoded, where 1 means that the ortholog gene is present and 0 means that it is not. Therefore if an observation is $\\leq$ 0.5 for those thresholds then it means this gene is not present and we would follow the left arrow.\n",
        "\n",
        "**NOTE:** TRUE is equivalent to **yes (left arrows)** and FALSE equivalent to **no (right arrows)** from the [image](#tree-methods) displayed at the begining of this notebook.\n",
        "\n",
        "***\n",
        "**B) Gini Impurity:** is a measure of purity of the classification. It is a number between 0 and 1, where a value of 0 means that all samples belong to one particular class (purest node); whereas a value of 1 means all samples are distributed equally among the classes (least pure node). Here is a link about [Gini Impurity](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity), if you are interested in the calculation and specific formula behind it.\n",
        "\n",
        "In our graph, we see that the root node has <font face=\"monospace\">Gini = 0.343</font> and as we go lower on the tree this value also lowers. In the leaf node we see <font face=\"monospace\">Gini = 0.0</font> because the remaining observations in the leaf belong to only one class, the Resistance (R) one. This means that all the observations that have the gene <font face=\"monospace\">group_6670</font>(root node) but not the gene <font face=\"monospace\">group_17551</font>(intermediate node) are classified exclusively as Resistant in this particular tree of our Random Forest model.\n",
        "***\n",
        "**C) Samples:** Is the number of unique observations from the original training dataset before the bagging proceedure. In the image we can see that the intermediate node with the threshold group_17551 $\\leq$ 0.5 has 41 unique samples and it is later split into a leaf node with 37 unique samples and another intermediate node with 4 samples (41 = 37 + 4).\n",
        "***\n",
        "**D) Values:** is a list of numbers showing the amount of actual observations classified AFTER the bagging proceedure (random sampling of training observations with replacement + aggregation). We only have 2 classes, so the length of this list is just 2 numbers, we can expect a list of 3 if there are 3 classes to classify, 4 if there are 4 classes, etc.\n",
        "\n",
        "In the image we can see that as we go down the decision tree the Resistant observations (in red) in the node with the Decision threshold <font face=\"monospace\">hsdR_1 $\\leq$ 0.5</font> has <font color=\"red\">253</font> observations classified  as Resistant, then uppon splitting into 2 intermediate nodes this value is also split into <font color=\"red\">133</font> and <font color=\"red\">120</font>. The same thing happens for the Susceptible split in this node <font color=\"blue\">(1122 = 994 + 128) </font>.\n",
        "\n",
        "**NOTE:** Usually for a regular Decision Tree the sum of the list of numbers in values would add up to the sample number BUT in the case of a Random Forest Decision Tree the **Samples** only count the unique observations, whereas our **Values** include the repeated copies from the bagging proceedure as well .\n",
        "***\n",
        "If you are not sure which number corresponds to a particular class, below we can access the index of the values shown in each node. As we can see the first index is Resistant (0) and the second index is Susceptible (1):"
      ],
      "metadata": {
        "id": "igi_iCYchOWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# line to see what classes belong to a particular index in the Values.\n",
        "RF_CTX_G_model.classes_"
      ],
      "metadata": {
        "id": "fNT6Xpn6Na5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6) Making predictions from Random Forest model**\n",
        "\n",
        "The code chunks below will attempt to show us how Random Forests make their predictions using in our case 100 Decision Trees. An important distinction between a Decision Tree and Random Forest is to consider how the trees are generated within the Random Forest. In Random Forest there are 2 randomizations happening when they create the trees:\n",
        "- Random selection of observations from the total training data (rows)\n",
        "- Random selection of features from the feature combination we have chosen, in this case it would be G (columns)\n",
        "\n",
        "**NOTE:** Same code as in previous notebook"
      ],
      "metadata": {
        "id": "1px3jn-Djn77"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzEGZrwHAZ3W"
      },
      "outputs": [],
      "source": [
        "# creating a function using the model created and trained and the feature combinations from testing data\n",
        "def predict(RF_combo_Model, features_test):\n",
        "  feat = features_test.drop(columns=[\"MLST\"])\n",
        "  labels_pred = RF_combo_Model.predict(feat)\n",
        "  if is_numeric_dtype(labels_pred):\n",
        "    # tranforming labels from numbers to letters\n",
        "    lab_pred_t = labels_pred.astype('O')\n",
        "    lab_pred_t[lab_pred_t==0] = 'R'\n",
        "    lab_pred_t[lab_pred_t==1] = 'S'\n",
        "    return lab_pred_t\n",
        "  else:\n",
        "    return labels_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we implement first the function \"combo_feat\" to specify which testing features we want to use to make our predictions. Then we use our predict function to finally get the predictions made by the Random Forest."
      ],
      "metadata": {
        "id": "QG7sFPN6poIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing combo_feat() function created for testing data\n",
        "CTX_G_test_df = combo_feat(CTX_Train_test_dic['features_test'],\"CTX\",\"G\")\n",
        "\n",
        "# Implementation of the predict() function using the feature combination \"GY\"\n",
        "CTX_G_labels_pred = predict(RF_CTX_G_model,CTX_G_test_df)\n",
        "\n",
        "# observe how many predictions were made for each category \"R\" and \"S\"\n",
        "print(\"Labels predicted: \", np.unique(CTX_G_labels_pred, return_counts=True))"
      ],
      "metadata": {
        "id": "JqaBG4rPuH7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final predictions from the Random Forests were 65 Resistant and 296 Susceptible. However to see how the final decisions were made, we can check how each tree makes predictions for **just the first observation** in the features test data. The code below provides a counter for trees that voted Susceptible vs the ones that voted Resistant for the third observation."
      ],
      "metadata": {
        "id": "22-CAkX6qHYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code below shows the number of trees that predicted one of the two outcomes\n",
        "votes_for_R = 0 #we start with 0 trees \"voting\" for R\n",
        "votes_for_S = 0 #and 0 trees \"voting\" for S\n",
        "\n",
        "#loop over each of the trees in the random forest\n",
        "for i in range(0,len(RF_CTX_G_model)):\n",
        "  #get a prediction from one tree\n",
        "  pred = RF_CTX_G_model.estimators_[i].predict(CTX_G_test_df.iloc[0,1:17199].to_numpy().reshape(1,-1))\n",
        "\n",
        "  # if the prediction is 0 then it is \"R\"\n",
        "  if pred == 0.00:\n",
        "    votes_for_R = votes_for_R + 1\n",
        "\n",
        "  #if the prediction is 1 then it is \"S\"\n",
        "  else:\n",
        "    votes_for_S = votes_for_S + 1\n",
        "# Counting the number of trees that predicted R and the ones that predicted S\n",
        "print(\"Number of trees that voted Resistant: \",votes_for_R)\n",
        "print(\"Number of trees that voted Susceptible: \",votes_for_S)"
      ],
      "metadata": {
        "id": "0QBks2vNuFx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we can check the prediction by the Random Forest, for the first observation as well. Because the majority of the trees voted for Resistance (R), we can see that our Random Forest ends up predicting R as well."
      ],
      "metadata": {
        "id": "LvrYcBp4Bozr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction using the second value of our test features\n",
        "pred = RF_CTX_G_model.predict(CTX_G_test_df.iloc[0,1:17199].to_numpy().reshape(1,-1))\n",
        "print(\"Prediction by Random Forest: \", pred)"
      ],
      "metadata": {
        "id": "5-WErPCU0Jfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7) Evaluating our model using a confusion matrix and metrics**\n",
        "\n",
        "**a) Creating evaluate() function and implementing it:**\n",
        "\n",
        "Similarly to the previous notebook, we will evaluate our RF model by using a Confusion Matrix and respective metrics. Below is a quick review of these, remember that there is one Accuracy score, but Recall and Presicion should have as many sets as classes our model its trained to predict:\n",
        "\n",
        "|<font size=3>Metrics|<font size=3>General formula|<font size=3> Formula for 2 classes|\n",
        "|--|:-:|:-:\n",
        "|<font size=3>**Accuracy**|<font size=3>$\\frac{Correctly \\ classified}{All \\ Predicted}$|<font size=3>$\\frac{TP + TN}{TP + TN + FN + FP}$|\n",
        "|<font size=3>**R Recall:**|<font size=3>$\\frac{Correctly \\ classified \\ as \\ R}{All \\ Actual \\ R}$|<font size=3>$\\frac{TP}{TP + FN}$|\n",
        "|<font size=3>**R Precision:**|<font size=3>$\\frac{Correctly \\ classified \\ as \\ R}{All \\ Predicted \\ R}$|<font size=3>$\\frac{TP}{TP + FP}$|"
      ],
      "metadata": {
        "id": "5dox3JfFn5wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function that evaluates our model using our actual and predicted data\n",
        "def evaluate(RF_combo_model, labels_test, labels_pred, cf= True, show_results=True):\n",
        "  report = classification_report(labels_test, labels_pred, output_dict = True)\n",
        "  if cf == True:\n",
        "    cm = confusion_matrix(labels_test, labels_pred, labels=np.unique(labels_pred))\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(labels_pred))\n",
        "    disp.plot()\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "  if show_results == True:\n",
        "    print(\"Results\")\n",
        "    print('Accuracy:',report['accuracy'])\n",
        "    print('R recall:',report['R']['recall'])\n",
        "    print('S recall:',report['S']['recall'])\n",
        "    print('R precision:',report['R']['precision'])\n",
        "    print('S precision:',report['S']['precision'])\n",
        "  return [report['accuracy'], report['R']['recall'], report['S']['recall'], report['R']['precision'], report['S']['precision']]"
      ],
      "metadata": {
        "id": "YJJmoOq-o97t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implementing the evaluate() function\n",
        "Model_Report = evaluate(RF_CTX_G_model, CTX_Train_test_dic['labels_test'],CTX_G_labels_pred)"
      ],
      "metadata": {
        "id": "sP4N-azGuL0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b) Create a feature importance graph showing which features were the most important**\n",
        "\n",
        "The gini impurity is a metric used to determine which features are more important than others. Below we will learn to graph the feature importance for the model we have been working on."
      ],
      "metadata": {
        "id": "2-6NzvrsbZaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting feature importance for our Random Forest model trained using G feature combo for CTX drug\n",
        "importance = RF_CTX_G_model.feature_importances_\n",
        "\n",
        "# getting the 20 highest feature importance out of all G features\n",
        "indices = np.argsort(importance)[-20:]\n",
        "highest_feat_importance = importance[indices]\n",
        "\n",
        "# create a barchart of the 20 most important features\n",
        "sns.set_theme()\n",
        "plt.barh([x for x in range(len(highest_feat_importance))], highest_feat_importance, tick_label=CTX_G_train_df.iloc[:,1:17199].columns[indices])\n",
        "plt.title(\"20 most important features for CTX_G_model\")\n",
        "\n",
        "# saving feature importance bargraph and display in notebook\n",
        "plt.savefig('/content/drive/My Drive/EColi_ML_Plots/RF_CTX_G_feat_importance.jpg',dpi=400, bbox_inches=\"tight\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TX0HY7tiuRgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the feature importance bar chart it seems that considering all Accessory genes, the gene **wbuC** seems to have the highest importance in this particular model for the antibiotic CTX. This probably means that strains with this gene are more likely to be resistant to the drug Cefotaxime (CTX).\n",
        "\n",
        "Note that this doesn't mean that wbuC causes resistance. [This paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8265643/) (fig 2) shows that wbuC may be located right next to the mcr1 gene on a plasmid. The association with mcr1 may be what makes wbuC a good predictor for resistance."
      ],
      "metadata": {
        "id": "z7rCiJS5cXne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8) (OPTIONAL) Hyperparameter Tuning and Crossvalidation for RF model**\n",
        "\n",
        "This section of the notebook is optional.\n",
        "Here we will carry out hyperparameter tuning using cross-validation to optimize the model's performance and ensure its robustness. We will also include code to use a blocked cross-validation design. These approaches are important to make good ML models. However, if you are just here to learn the basics, feel free to skip section 8."
      ],
      "metadata": {
        "id": "y2qYJYLfGzHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **a) Hyperparameter Tuning**\n",
        "\n",
        "The results using our RF model with default features seem to already perform quite well, However we can always see if it can do better, specially for R recall score. For a complete list of of the hyperparameters available for Random Forests click [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). This time we will tune the number of estimators (n_estimators). A higher number of trees decreases the risk for overfitting as it does not increase generalization error based on the mathematical details of this model (which we won't discuss here). It is important to note that the more number of trees the higher the computational resources use, most of the time people leave the number of trees to be the default value (100), however just as an example we have listed up to 300 trees."
      ],
      "metadata": {
        "id": "K0JI0ekV-XXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# performing hyperparameter tuning for model\n",
        "## Creating dictionary of parameters to tune.\n",
        "hparam = {\"n_estimators\":[50, 100, 200, 300]}"
      ],
      "metadata": {
        "id": "2O2Q4XQFEvmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **b) Crossvalidation**"
      ],
      "metadata": {
        "id": "QCICB3_KG4Dl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating function to perform hyper parameter tuning of model\n",
        "def RF_hp_tune(param, feat_train_df, lab_train, v=3, cv=4):\n",
        "  # creating Random Forest model\n",
        "  model = RandomForestClassifier(random_state = 42)\n",
        "  # performing the hyper parameter tuning using crossvalidation\n",
        "  scoring_dic = {'f1_macro':make_scorer(f1_score , average='macro')}\n",
        "  # isolating the features used for training\n",
        "  feat = feat_train_df.drop(columns=[\"MLST\"])\n",
        "  # tranforming labels from letters to 0 & 1\n",
        "  lab_train_t = lab_train.replace({'R': 0, 'S': 1})\n",
        "  # Fitting using grid search parameters depending on crossvalidation scheme used\n",
        "  if str(cv).isnumeric():\n",
        "    cv = KFold(cv)\n",
        "    gs = GridSearchCV(model, param, scoring=scoring_dic,cv=cv, refit='f1_macro', verbose=v, return_train_score=True)\n",
        "    gs.fit(feat, lab_train_t)\n",
        "  elif cv == \"blocked\":\n",
        "    groups= feat_train_df['MLST']\n",
        "    cv = StratifiedGroupKFold(n_splits=4)\n",
        "    gs = GridSearchCV(model, param, scoring=scoring_dic,cv=cv, refit='f1_macro', verbose=v, return_train_score=True)\n",
        "    gs.fit(feat, lab_train_t, groups=groups)\n",
        "  else:\n",
        "    print(\"Please provide a valid crossvalidation scheme `blocked` or an integer\")\n",
        "  print(gs.best_params_)\n",
        "  print(gs.best_score_)\n",
        "  return gs.best_estimator_"
      ],
      "metadata": {
        "id": "9wYf0W8pHvl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the function created (takes longer time)\n",
        "RF_tuned = RF_hp_tune(hparam, CTX_G_train_df, CTX_Train_test_dic['labels_train'])"
      ],
      "metadata": {
        "id": "DkZiOx4jJpbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case we see that in terms of number of trees used for the Random Forest, the best average f1-score for R and S is obtained with 200 trees, when the model is using only the Accessory genes presence/absence features (**G**) to make predictions for CTX."
      ],
      "metadata": {
        "id": "6DECYfPpKteJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **9) Use all functions and evaluate every drug in every feature combination!**\n",
        "\n",
        "**NOTE:** Code below is the same as in previous notebook"
      ],
      "metadata": {
        "id": "N5lGr0hoDCip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **a) Lets recall the list of drugs we have available and the combination of features we are interested in**"
      ],
      "metadata": {
        "id": "yExXAdaWZL4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check all drugs\n",
        "drug_list = All_Drugs_df.iloc[:,3:15].columns\n",
        "print(drug_list)\n",
        "\n",
        "# let's see all combinations we are interested in\n",
        "print(combo_list)"
      ],
      "metadata": {
        "id": "51wJM2XUaxtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **b) Create a loop that will go through all our functions using the lists above**"
      ],
      "metadata": {
        "id": "cZWO1rYocRQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for fname in os.listdir(filepath):\n",
        "    if fname.endswith('RF_metrics_df.csv'):\n",
        "        print(\"A csv with stored results for Random Forest has already been created. Please check your Google Drive directory.\")\n",
        "        break\n",
        "else:\n",
        "  # Lets use all our functions this time and save our report into a single data structure\n",
        "  RF_model_metrics = {}\n",
        "\n",
        "  for drug in drug_list:\n",
        "    print(drug)\n",
        "    # splits each drug df into a dictionary with testing and training data\n",
        "    Test_Train_dic = Split_train_test_antibiotic(drug)\n",
        "    for combo in combo_list:\n",
        "      # Training each drug and combo features\n",
        "      labels_train = Test_Train_dic[\"labels_train\"]\n",
        "      # create corresponding feature_df for training\n",
        "      features_train = combo_feat(Test_Train_dic[\"features_train\"], drug, combo)\n",
        "      print(drug+\"_\"+combo)\n",
        "\n",
        "      # runs RF model using the corresponding training feature_df and does hyperparameter tuning to get best model\n",
        "      RF_combo_model = run_RF(features_train, labels_train, drug, combo)\n",
        "\n",
        "      #Optional: If you want to use the hyperparamter tuning step 8 use this line instead of the previous one:\n",
        "      #RF_combo_model = RF_hp_tune(hparam, features_train, labels_train, v=0)\n",
        "\n",
        "      # create corresponding feature_df for testing\n",
        "      features_test = combo_feat(Test_Train_dic[\"features_test\"], drug, combo)\n",
        "      # generate predictions based on the feature combination tested\n",
        "      labels_pred = predict(RF_combo_model, features_test)\n",
        "      # loading the actual labels\n",
        "      labels_test = Test_Train_dic[\"labels_test\"]\n",
        "      # Evaluating our model\n",
        "      report = evaluate(RF_combo_model, labels_test, labels_pred, cf=False, show_results=False)\n",
        "      # Saving the results into a dictionary\n",
        "      RF_model_metrics[drug+\"_\"+combo] = report\n",
        "      print(report)\n",
        "  # convert dictionary into a dataframe\n",
        "  RF_metrics = pd.DataFrame.from_dict(RF_model_metrics, orient='index',columns=[\"Accuracy\", \"R_recall\", \"S_recall\", \"R_precision\", \"S_precision\"]).reset_index()\n",
        "  RF_metrics = RF_metrics.rename(columns = {'index':'Drug_combo'})\n",
        "\n",
        "  # saving our metric results into a CSV file\n",
        "  RF_metrics.to_csv(filepath+\"RF_metrics_df.csv\", index= False)"
      ],
      "metadata": {
        "id": "V-TyWAOfwa-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **10) Create a bar graph showing metrics for all drugs using only G features**\n",
        "\n",
        "Using the CSV we created above with all our results, we will create a bargraph to visualize metrics for every antibiotic that uses the G feature (Gene Presence and Absence). In the graph below we are comparing Accuracy and Recall scores for both classes.\n",
        "\n",
        "The code below serves to create a new directory to store all our Machine Learning plots and figures created. If you run the code more than once it will let you know that the figure has already been created in the directory."
      ],
      "metadata": {
        "id": "m8CQMKARZ1zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # makes a directory for all your plot images\n",
        "  os.mkdir('/content/drive/My Drive/EColi_ML_Plots')\n",
        "except:\n",
        "  print(\"A directory was already created to store your plot\")\n",
        "\n",
        "# filtering for all the rows that contain GY features\n",
        "RF_metrics = pd.read_csv(\"/content/drive/MyDrive/EColi_ML_CSV_files/RF_metrics_df.csv\")\n",
        "G_filter = [drug_combo for drug_combo in RF_metrics['Drug_combo'] if drug_combo.endswith(\"G\")]\n",
        "G_df = RF_metrics.loc[RF_metrics[\"Drug_combo\"].isin(G_filter)]\n",
        "\n",
        "# Figure Size\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "fig = plt.figure(figsize =(15, 6))\n",
        "\n",
        "# Adding title\n",
        "plt.title('Accuracy, R_recall, S_recall', fontsize = 12)\n",
        "\n",
        "# Variables to be plotted\n",
        "x = np.arange(len(G_df[\"Drug_combo\"]))\n",
        "acc = list(G_df[\"Accuracy\"])\n",
        "R_rec = list(G_df[\"R_recall\"])\n",
        "S_rec = list(G_df[\"S_recall\"])\n",
        "\n",
        "# Plotting barcharts\n",
        "acc_bar=plt.bar(x-0.25, height= acc, width=0.25, color=\"grey\", edgecolor=\"gray\")\n",
        "rrec_bar=plt.bar(x, height= R_rec, width=0.25, color=\"cadetblue\", edgecolor=\"gray\")\n",
        "srec_bar=plt.bar(x+0.25, height= S_rec, width=0.25, color=\"powderblue\", edgecolor=\"gray\")\n",
        "\n",
        "plt.xticks([r for r in range(len(G_df[\"Drug_combo\"]))],\n",
        "            G_df[\"Drug_combo\"], fontsize = 12)\n",
        "\n",
        "#legend\n",
        "fig.legend([acc_bar,rrec_bar,srec_bar],[\"Accuracy\", \"R_recall\", \"S_recall\"], bbox_to_anchor=(0.4,-0.35, 0.04, 0.4), fontsize=12)\n",
        "\n",
        "# Saving bargraph into our new directory\n",
        "plt.savefig('/content/drive/My Drive/EColi_ML_Plots/RF_G_Accuracy_and_Recall_Scores.jpg',dpi=400, bbox_inches=\"tight\")\n",
        "\n",
        "# Show Plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gCCYk_k7eKA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! We have created and saved all the Random Forest results for every antibiotic and combination of features we were interested in.\n",
        "\n",
        "***OPEN QUESTION:***\n",
        "\n",
        "Look at the graph and compare to the graph you made for the Logistic Regression model. Do the results look the same? If there is a difference, does RF do better or worse than LG?\n",
        "\n",
        "Now that we have a basic understanding of tree methods in general. It is time to learn about another tree method: [Extreme Gradient Boosted Tree](https://colab.research.google.com/drive/1quEjtE147vchUNEmJubZnvBJ3E1uk5Dk?usp=drive_link)."
      ],
      "metadata": {
        "id": "7b4ILsjdgrN9"
      }
    }
  ]
}
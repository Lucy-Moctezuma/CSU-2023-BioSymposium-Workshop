{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lucy-Moctezuma/ML-Tutorial-for-Antibiotic-Resistance-Predictions-for-E.-Coli/blob/main/2)_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(**Note:**\n",
        "Click on the button that reads *“Open in Colab”* to open this code in Google Colab. Once open in Google Colab, you can make a copy of the notebook in your personal drive and run the code by clicking a little triangle/arrow to the left of each code block.)"
      ],
      "metadata": {
        "id": "Rt_VNNpf3Q6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Logistic Regression**\n",
        "\n",
        "## ***Objectives for this Notebook***\n",
        "- Learn the basics of how logistic regression model works.\n",
        "- Create functions to implement Logistic Regression to Moradigaravand's dataset.\n",
        "\n",
        "**Logistic Regression** is a classification model that allows us to predict the probability for a binary outcome (2 classes). Typically the threshold for logistic regression is 0.5; In our example, this means that above this probability, the model would predict **R** (Resistant) and below this it will predict **S** (Susceptible).\n",
        "\n",
        "The equation for Logistic Regression is derived from Linear Regression, but instead of using the response variable Y, it employs the natural log of the odds:\n",
        "\n",
        "$$ ln(\\frac{P}{1-P}) = \\hat \\beta_0 + \\hat \\beta_jX$$\n",
        "\n",
        "After isolating P, we end up with the equation below:\n",
        "\n",
        "$$P = \\frac{e^{\\hat{\\beta}_{0}+\\hat{\\beta}_{j}X}}{1+e^{\\hat{\\beta}_{0}+\\hat{\\beta}_{j}X}}$$\n",
        "\n",
        "- ***P*** is the probability of an outcome. Therefore P is a number between 0(0%) and 1(100%). To make a binary prediction (Resistant or Susceptible), we use a threshold of 0.5. For our example if P < 0.5 our model would predict Susceptible (S) and if P > 0.5 our model would predict Resistant (R)\n",
        "\n",
        "- $\\hat \\beta_0$ is the intercept term and $\\hat \\beta_j = [\\beta_1 , \\beta_2 , \\beta_3, ... , \\beta_{17199}]$ are the coefficients for each of our features, which the model will try to estimate using our data, there is one coefficient per column feature and we are estimating 17199 (!) of them in our example.\n",
        "\n",
        "- $X = [Year \\ column + Gene \\ Absence \\ and \\ Presence \\ columns]$\n",
        "\n",
        "We will see each of the parts of this equation as we go along in the tutorial, so we can have better picture of these.\n",
        "\n",
        "A full understanding of the math behind the logistic regression model is not necessary. Using a logistic regression model (or any other machine learning model) doesn't require a detailed understanding of the math behind it. It is important however to know when to implement particular models. For instance, logistic regression is usually suitable for predicting binary labels, such in this case, where we have to determine either R or S for each isolate.\n",
        "\n"
      ],
      "metadata": {
        "id": "4s_KUXfGSthq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1) Importing Packages needed**\n"
      ],
      "metadata": {
        "id": "fnpxHzYUUJ_M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDa820s0JSoJ"
      },
      "outputs": [],
      "source": [
        "# Data manipulation imports for ML\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas.api.types import is_numeric_dtype\n",
        "\n",
        "# Import packages for logistic regression model and hyperparameter tuning\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.utils._testing import ignore_warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Imports for model evaluation\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, get_scorer_names\n",
        "from sklearn.metrics import f1_score, make_scorer, accuracy_score, recall_score, precision_score\n",
        "from sklearn.model_selection import StratifiedGroupKFold, KFold\n",
        "import warnings\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "warnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)\n",
        "\n",
        "# Imports for data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Imports for file management\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2) Loading CSV file and creating dataframes for each antibiotic**\n",
        "In here we will be loading the CSV we created in the previous notebook. This file should contain information on resistance to all the antibiotic drugs (Labels) and all the Years of isolation , Gene Presence/Absence data (Features) and the Sequence Type data.\n"
      ],
      "metadata": {
        "id": "1MTk8vozVJqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads csv file as a dataframe\n",
        "# If the file is \"not found\" go back to Notebook 1 to make sure you create the file in your Drive.\n",
        "filepath = '/content/drive/My Drive/EColi_ML_CSV_files/'\n",
        "\n",
        "# reads csv file as a dataframe\n",
        "All_Drugs_df = pd.read_csv(filepath+\"EColi_Merged_df.csv\", na_values=\"NaN\")\n",
        "All_Drugs_df.head()"
      ],
      "metadata": {
        "id": "psqfFp_yVsmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3) Separating each Drug Dataframe into 4 sections : Training features, training labels, testing features and testing labels.**\n",
        "\n",
        "The objective of this part will be to first create a single dataframe for each antibiotic drug and then split that data frame into 4 parts (see below). The dataframe will have all our features and the label for only one drug. This is because Resistance and Susceptibility are not universal. For example, just because an isolate of *E. coli* is resistant to say AMP (Ampicilin), it doesn't mean that is resistant to CIP (Ciprofloxacin). We will be training ML models for each of the antibiotics separately.\n",
        "\n",
        "Below we can check the list of antibiotics again:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NMKzeaqPojso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#here we make a list of the antibiotics in our combined dataframe\n",
        "drug_list = All_Drugs_df.iloc[:,3:15].columns\n",
        "drug_list"
      ],
      "metadata": {
        "id": "U40wWYTyjDtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After making the individual dataframes for each drug, we will split each of our 12 antibiotic dataframes into 4 different sections:\n",
        "\n",
        "**Two TRAINING sections**\n",
        "\n",
        "**a) labels_train:** are the labels (S or R) for a single antibiotic drug that will be used to teach our model how to make predictions.\n",
        "\n",
        "**b) features_train:** are the features that will be used along with the labels_train to teach our model to make predictions. Note that this is actually the X matrix in our logistic equation! They will be used to estimate our $\\beta_0$ and all the $\\beta_j$ values with a process called *Maximum Likelihood*. You can watch the mathematical details of how this is done by watching this [video](https://www.youtube.com/watch?v=BfKanl1aSG0) by Josh Starmer\n",
        "\n",
        "**Two TESTING sections**\n",
        "\n",
        "**c) labels_test:** are the labels we will holding out so that we can see at the end if we made accurate predictions.\n",
        "\n",
        "**d) features_test:** are the X values we will plug into our model, once $\\beta_0$ and all the $\\beta_j$ values have already been estimated.\n",
        "\n",
        "- Below we create a function that will be used to separate each of our 12 dataframes (for 12 drugs) into the 4 separate parts described above. We also specify that 20% of our data to be used as a testing set and thus 80% of our data remains to become our training set. You can choose a different percentage to split them, but know that the majority of our data should be used for training.\n",
        "\n",
        "In addition, the function we create to make these 4 sections will save the 4 parts into a python Dictionary object. If you are unfamiliar with what a dictionary is in python, feel free to check out this useful [link](https://docs.python.org/3/tutorial/datastructures.html#dictionaries). This way we can organize and access our 4 data chunks for a specific antibiotic drug.\n",
        "\n",
        "**The function created below will create a dataframe and split the data in Training (labels and features) and Testing (labels and features) for each antibiotic.**"
      ],
      "metadata": {
        "id": "pbH90vGNjBGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating each dataframe into Labels and Features for training and testing data.\n",
        "# Our function uses the handy train_test_split() function.\n",
        "\n",
        "def Split_train_test_antibiotic(drug):\n",
        "  #here we make a list of the columns we want to keep: the column for the isolate, the column for the drug we are interested in and all features (starting from column 15).\n",
        "  df_list = [All_Drugs_df[[\"MLST\",\"Isolate\",drug,\"Year\"]], All_Drugs_df.iloc[:,15:]]\n",
        "\n",
        "  #here we create a data frame with just the columns we wanted to keep.\n",
        "  Drug_df = pd.concat(df_list, axis=1)\n",
        "\n",
        "  #here we drop all rows with missing data\n",
        "  Drug_df = Drug_df.dropna()\n",
        "\n",
        "  # Creating a dictionary to store each antibiotic datasets\n",
        "  Train_test_dic = {}\n",
        "\n",
        "  # Defining the label columns\n",
        "  labels = Drug_df[drug]\n",
        "\n",
        "  # Defining features columns\n",
        "  features = Drug_df.drop(columns=[drug])\n",
        "\n",
        "  # Separating training (features and labels) and testing (features and labels) datasets\n",
        "  # We use stratify=labels so that the fraction resistant samples is the same in the test and the train sections\n",
        "  features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.20, random_state=42, stratify=labels)\n",
        "\n",
        "  # storing each data chunk in a dictionary\n",
        "  Train_test_dic['labels_train'] = labels_train\n",
        "  Train_test_dic['features_train'] = features_train\n",
        "  Train_test_dic['labels_test'] = labels_test\n",
        "  Train_test_dic['features_test'] = features_test\n",
        "\n",
        "  return Train_test_dic"
      ],
      "metadata": {
        "id": "R61HHXJCX6Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the function Split_train_test_antibiotic() for CTZ example\n",
        "CTZ_Train_test_dic = Split_train_test_antibiotic(\"CTZ\")\n",
        "\n",
        "# checking the shape of each dataframe or series stored in the dictionary created for drug CTZ\n",
        "print(\"CTZ\")\n",
        "for k, df in CTZ_Train_test_dic.items():\n",
        "  print(k, df.shape)"
      ],
      "metadata": {
        "id": "E26gr3ULdMEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we can see a count of Susceptible and Resistance strains for the Training and Testing datasets. Notice that there are a lot more Susceptible than Resistant *E. coli* isolates to CTZ drug. This is good news for us as humans (most *E. coli* are susceptible to most drugs), but for ML approaches it can be a problem. This is considered an \"inbalanced\" dataset. Later on we will discuss how certain metrics may not be as reliable as others when looking at an imbalanced dataset."
      ],
      "metadata": {
        "id": "DIcTBpVQRSya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accessing a particular chunk of data\n",
        "print(\"Class Counts for Training Dataset:\")\n",
        "CTZ_Train_test_dic[\"labels_train\"].value_counts()"
      ],
      "metadata": {
        "id": "fcKGS4Ex4AtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accessing a particular chunk of data\n",
        "print(\"Class Counts for Testing Dataset:\")\n",
        "CTZ_Train_test_dic[\"labels_test\"].value_counts()"
      ],
      "metadata": {
        "id": "qumUgwdy4vUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4) Creating different combinations of features before training**\n",
        "\n",
        "The next part of this project is to add some complexity in our analysis by choosing specifically what sort of features we would like to include.\n",
        "\n",
        "Recall that we have 2 types of features we went on detail on our [first notebook](https://colab.research.google.com/drive/13SbCF3LFXwM_jZELxQA4aBHUvPgfmzzP?usp=sharing):\n",
        "\n",
        "- **Y**: Years of Isolation\n",
        "- **G**: Gene presence and absence\n",
        "\n",
        "We are interested in the following combinations: **G, Y** and **GY** This means that for any possible combination of feautures, we will train and test the machine learning models.\n",
        "\n",
        "Below we create a function that will take the features dataframe (train or test) from the dictionary we have created in part 3 and then create dataframes with different feature combinations ."
      ],
      "metadata": {
        "id": "2IyqN506dQn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making a list of combinations of data sources we would like to test in our ML models\n",
        "combo_list = ['Y', 'G', 'GY']\n",
        "\n",
        "# making a function that creates different feature combinations of the predictor features\n",
        "def combo_feat(features_df, drug, combo):\n",
        "\n",
        "  # Isolating Year as a feature and\n",
        "  year_filter = [col for col in features_df if col.startswith(\"Year\")]\n",
        "  year_feat = features_df[year_filter]\n",
        "\n",
        "  # creating Gene precence column filters for features_df\n",
        "  gene_presc_filter = [col for col in features_df.columns if col not in year_filter and col != \"Isolate\"]\n",
        "  gene_presc_feat = features_df[gene_presc_filter]\n",
        "\n",
        "  # Note that we also make sure here that every set of features has the MLST column included.\n",
        "  # We will drop that column later on when we train our models.\n",
        "  # The reason we keep it here is so that we can use it later to do a \"blocked cross validation\" in step 8.\n",
        "\n",
        "  if combo == 'Y':\n",
        "    df_list = [features_df[['MLST','Isolate']], year_feat]\n",
        "    Y_feat_df = pd.concat(df_list, axis=1)\n",
        "    Y_feat_df = Y_feat_df.drop(columns=['Isolate'])\n",
        "    return Y_feat_df\n",
        "\n",
        "  if combo == 'G':\n",
        "    df_list = [features_df['Isolate'], gene_presc_feat]\n",
        "    G_feat_df = pd.concat(df_list, axis=1)\n",
        "    G_feat_df = G_feat_df.drop(columns=['Isolate'])\n",
        "    return G_feat_df\n",
        "\n",
        "  if combo == 'GY':\n",
        "    df_list = [features_df['Isolate'], year_feat, gene_presc_feat]\n",
        "    GY_feat_df = pd.concat(df_list, axis=1)\n",
        "    GY_feat_df = GY_feat_df.drop(columns=['Isolate'])\n",
        "    return GY_feat_df"
      ],
      "metadata": {
        "id": "JmzbwQBxlylk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing combo_feat() function created for training data\n",
        "CTZ_GY_train_feat = combo_feat(CTZ_Train_test_dic['features_train'],\"CTZ\",\"GY\")\n",
        "\n",
        "# looking only at the feature column names for the combination for \"GY\" for drug \"CTZ\" for training data\n",
        "CTZ_GY_train_feat.columns"
      ],
      "metadata": {
        "id": "sqvvdtJOu_RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTES:**\n",
        "\n",
        "- We have **MLST** Sequence Type as a possible feature, but we will not use it as a feature in this tutorial. We will drop it for training the model.\n",
        "\n",
        "- Later we can choose to use MLST for cross-validation purposes in order to increase generalizability of the ML models and avoid overfitting. This is important to consider because isolates that might have come from the same place, patient, etc. might have similar genetic composition and thus not be independent from each other.\n"
      ],
      "metadata": {
        "id": "-rnDaATPjkiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5) Creating and Running Logistic regression model**\n",
        "\n",
        "The next step involves creating a function that will actually create our Logistic Regression model and train it on our desired combination of training features and the labels for the drug we choose. While this function seems fairly straight forward, there is a lot of calculations happening in the backrgound when we call the LG.fit() function which trains our model. Notice that in this example function we are not considering the **MLST** column. We will only take a small peek at what's going on in the background.\n"
      ],
      "metadata": {
        "id": "ZVOKCEZNeHzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating Logistic regression model function\n",
        "@ignore_warnings(category=ConvergenceWarning)\n",
        "def run_LG(feat_train_df, lab_train, drug, combo):\n",
        "  feat=feat_train_df.drop(columns=['MLST'])\n",
        "  print(drug +\" Training combo: \"+ combo)\n",
        "  LG = LogisticRegression(random_state = 42, max_iter=500, class_weight='balanced')\n",
        "  return LG.fit(feat, lab_train)"
      ],
      "metadata": {
        "id": "QbEP2WYkWPxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implementing run_LG() for specific drug feature combination dataframe\n",
        "LG_CTZ_GY_model = run_LG(CTZ_GY_train_feat, CTZ_Train_test_dic['labels_train'],\"CTZ\",\"GY\")\n",
        "LG_CTZ_GY_model"
      ],
      "metadata": {
        "id": "_HYfVIgUvEWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# looking at what classes the model is able to predict\n",
        "LG_CTZ_GY_model.classes_"
      ],
      "metadata": {
        "id": "Mt-5wtNNV7l3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we can see that our model contains several coefficients. We will printout first the intercept ($\\hat{\\beta}_{0}$), then a list of the coefficients that correspond to each column feature ($\\hat{\\beta}_{j}$) and finally we can get a print out of the total number of coefficients in our model, these should be the same number as all the features we used in this example."
      ],
      "metadata": {
        "id": "ynlnJ_21mH7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# printing the beta_0 or intercept value of our model\n",
        "print(\"Intercept:\",LG_CTZ_GY_model.intercept_[0])\n",
        "\n",
        "# printing all the beta_j's or coefficients of our logistic regression model\n",
        "print(\"All beta_j values:\", LG_CTZ_GY_model.coef_[0])\n",
        "\n",
        "# printing the number of all the beta_j values\n",
        "print(\"Number of beta_j values: \", len(LG_CTZ_GY_model.coef_[0]))"
      ],
      "metadata": {
        "id": "t99qNo2bvF8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6) Making predictions from Logistic regression model**\n",
        "\n",
        "Now that our model has been trained and all $\\beta$ values have been estimated, we are ready to make predictions! We will use the features of our testing data which we separated when we made our antibiotic drug dictionary.\n",
        "\n",
        "Below we create another function where we predict labels using the actual model and the \"features_test\" chunk.\n"
      ],
      "metadata": {
        "id": "1px3jn-Djn77"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzEGZrwHAZ3W"
      },
      "outputs": [],
      "source": [
        "# creating a function using the model created and trained and the feature combinations from testing data\n",
        "def predict(LG_combo_Model, features_test):\n",
        "  feat = features_test.drop(columns=['MLST'])\n",
        "  labels_pred = LG_combo_Model.predict(feat)\n",
        "  if is_numeric_dtype(labels_pred):\n",
        "    # tranforming labels from numbers to letters\n",
        "    lab_pred_t = labels_pred.astype('O')\n",
        "    lab_pred_t[lab_pred_t==0] = 'R'\n",
        "    lab_pred_t[lab_pred_t==1] = 'S'\n",
        "    return lab_pred_t\n",
        "  else:\n",
        "    return labels_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we will use the function **combo_feat()** to split the testing dataset"
      ],
      "metadata": {
        "id": "wwYUz_D4Un57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing combo_feat() function created for testing data\n",
        "CTZ_GY_test_df = combo_feat(CTZ_Train_test_dic['features_test'],\"CTZ\",\"GY\")"
      ],
      "metadata": {
        "id": "xifjI6iHvK66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The we will implement the function **predict()** below:"
      ],
      "metadata": {
        "id": "kYZfi84wmweT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of the predict() function using the feature combination \"GY\"\n",
        "CTZ_GY_labels_pred = predict(LG_CTZ_GY_model,CTZ_GY_test_df)\n",
        "\n",
        "# observe how many predictions were made for each category \"R\" and \"S\"\n",
        "print(\"Labels predicted: \", np.unique(CTZ_GY_labels_pred, return_counts=True))"
      ],
      "metadata": {
        "id": "EE1fy63XvI0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see in the last output that index order for **R (Resistance is 0)** and for **S (Susceptible is 1)**, meaning that we predicted 42 Resistant and 334 Susceptible samples."
      ],
      "metadata": {
        "id": "fi1DXXxfwUzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the predictions for the first 30 isolates\n",
        "print(\"Labels predicted for first 10 test isolates: \", CTZ_GY_labels_pred[:30])\n",
        "\n",
        "# and the actual labels for the first 30 isolates – do they match?\n",
        "print(\"Labels predicted for first 10 test isolates: \", np.array(CTZ_Train_test_dic['labels_test'][:30]))\n"
      ],
      "metadata": {
        "id": "_QGUYMhg5Gy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7) Evaluating our model using a confusion matrix and metrics**\n",
        "There are different ways we can evaluate our model. A **confusion matrix** is a plot showing a prediction summary for our model. It allows us to see how many predictions were correct and incorrect. There are also different metrics we can calculate using this graph. For this tutorial we will focus on three of them: **Accuracy**, **Recall** and **Precision**.\n",
        "\n",
        "- **Accuracy:** is the total number of correct classifications over the total amount of predictions made.\n",
        "\n",
        "- **Recall:** is the number of correct classifications made for a particular class over all predictions of that class.\n",
        "\n",
        "- **Presicion:** is the number of classifications made for a particular class over the actual number of strains for that class.\n",
        "\n",
        "Recall and Precision can each be calculated for resistance and for susceptibility.\n",
        "\n",
        "When we have two classes, a 2 by 2 confusion matrix contains:\n",
        "\n",
        "- **True Positives (TP)** = Resistant strains correctly classified as resistant (R) = 34\n",
        "- **True Negatives (TN)** = Susceptible strains correctly classified as susceptible (S) = 316\n",
        "- **False Positives (FP)** = Susceptoble strains incorrectly classified as resistant (R) = 8\n",
        "- **False Negatives (FN)** = Resistant strains incorrectly classified as susceptible (S) = 18"
      ],
      "metadata": {
        "id": "5dox3JfFn5wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function that evaluates our model using our actual and predicted data\n",
        "def evaluate(LG_combo_model, labels_test, labels_pred, cf= True, show_results=True):\n",
        "  report = classification_report(labels_test, labels_pred, output_dict = True)\n",
        "  if cf == True:\n",
        "    cm = confusion_matrix(labels_test, labels_pred, labels=np.unique(labels_pred))\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(labels_pred))\n",
        "    disp.plot()\n",
        "    plt.show()\n",
        "  if show_results == True:\n",
        "    print(\"Results\")\n",
        "    print('Accuracy:',report['accuracy'])\n",
        "    print('R recall:',report['R']['recall'])\n",
        "    print('S recall:',report['S']['recall'])\n",
        "    print('R precision:',report['R']['precision'])\n",
        "    print('S precision:',report['S']['precision'])\n",
        "  return [report['accuracy'], report['R']['recall'], report['S']['recall'], report['R']['precision'], report['S']['precision']]"
      ],
      "metadata": {
        "id": "YJJmoOq-o97t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we implement our function we can show a manual way in which these metrics are calculated, first for the overall *Accuracy* and then *Recall* and *Precision* for only the Resistant strains (R) as an example:\n",
        "\n",
        "|<font size=3>Metrics|<font size=3>General formula| <font size=3>Formula for 2 classes|<font size=3>Manual Calculation|\n",
        "|--|:-:|:-:|:--|\n",
        "|<font size=3>**Accuracy**|<font size=3>$\\frac{Correctly \\ classified}{All \\ Predicted}$|<font size=3>$\\frac{TP + TN}{TP + TN + FN + FP}$|<font size=3>$\\frac{34 + 316}{376} = 0.931$|\n",
        "|<font size=3>**R Recall:**|<font size=3>$\\frac{Correctly \\ classified \\ as \\ R}{All \\ Actual \\ R}$|<font size=3>$\\frac{TP}{TP + FN}$|<font size=3>$\\frac{34}{34 + 18} = 0.654$|\n",
        "|<font size=3>**R Precision:**|<font size=3>$\\frac{Correctly \\ classified \\ as \\ R}{All \\ Predicted \\ R}$|<font size=3>$\\frac{TP}{TP + FP}$|<font size=3>$\\frac{34}{34 + 8} = 0.810$|\n",
        "\n",
        "\n",
        "**NOTE:** In this tutorial we only work with 2 classes (R and S), thus the abbreviations (TP, TN, FP and FN) apply to our confusion matrix, however for situations with more than 2 classes, refer to the general formula column."
      ],
      "metadata": {
        "id": "b0uqIS4V0Il4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# implementing the evaluate() function\n",
        "Model_Report = evaluate(LG_CTZ_GY_model, CTZ_Train_test_dic['labels_test'], CTZ_GY_labels_pred)"
      ],
      "metadata": {
        "id": "AM3fq7QqvNqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that in this example, accuracy is 93.09%. That sounds quite good. But 18 out of the 52 of the resistant cases were misclassified as susceptible (recall score for R is only 65.4%). So, if the main goal here is to detect resistant strains, this particular model does not perform so well."
      ],
      "metadata": {
        "id": "YNSVkF2G9bW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8) (OPTIONAL) Hyperparameter Tuning and Crossvalidation for LG model**\n",
        "\n",
        "This section of the notebook is optional.\n",
        "Here we will carry out hyperparameter tuning using cross-validation to optimize the model's performance and ensure its robustness. We will also include code to use a blocked cross-validation design. These approaches are important to make good ML models. However, if you are just here to learn the basics, feel free to skip section 8.\n",
        "\n"
      ],
      "metadata": {
        "id": "9GHhuszf-9Ka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **a) Hyperparameter Tuning**\n",
        "This is the process of modifying the way we train our data, every ML model has different hyperparameters we can tune in order to get the best results. The LG model used several default parameters that we could tune. For a complete list of hyperparameters and details of what each controls in the model, click [here.](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
        "\n",
        "However, to keep this tutorial simpler we will perform tuning for just the hyperparameter **\"C\"**, which controls the amount of regularization. Regularization is a method to prevent overfitting. Overfitting means that the model will try to make perfect predictions for the training dataset, but these predictions will not generalize to other data.\n",
        "With a small **\"C\"** the model will overfit less.\n",
        "\n",
        "We start first by creating a dictionary of the parameters we would like to tune in our logistic regression model. You can see below that we will try to test 4 different values for the hyperparameter **\"C\"** :"
      ],
      "metadata": {
        "id": "onoYJmwjEdsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# performing hyperparameter tuning for model\n",
        "## Creating dictionary of parameters to tune.\n",
        "hparam = {\"C\":[0.001, 0.01, 0.1, 1.00]}"
      ],
      "metadata": {
        "id": "UcwghBhUwgn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **b) Crossvalidation**\n",
        "\n",
        "In addition to regularization, there are other ways to prevent overfitting. Crossvalidation is a technique used during training to avoid overfitting. In it's simplest form the training dataset splits into different chunks of data, where each chunk takes turns to become a hold out chunk of data to be tested after training with the remaining chunks.\n",
        "\n",
        "There are different crossvalidation schemes. We will first focus on a regular 4-fold cross validation scheme.\n",
        "\n",
        "The function created below will be used to perform the **hyperparameter \"C\" tuning** using a regular **4-fold crossvalidation** scheme, meaning the training data will be randomly split in 4 chunks, where each of the chunks will take turns to be the a validation test set, while the remaining chunks are used for training. Therefore we will train 16 times, (4 values for \"C\" X 4 validation tests). The output of our function will be the best model found after the hyper parameter tuning and crossvalidation performed.\n",
        "\n",
        "#### **c) Blocked Crossvalidation**\n",
        "\n",
        "Some research papers within the field of population genetics prefer to take into account the population structure of the dataset, as several isolates may belong to the same sequence type, patient or phylogenetic group, therefore they prefer to apply crossvalidation schemes that perform group splits  (or blocked) crossvalidation. Here is a list of [different crossvalidation schemes](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators) available in sklearn.\n",
        "\n",
        "Others choose to reweight isolates depending on their group size, in order to give less importance to samples from the same group.\n",
        "\n",
        "Here we will show how we can use the MLST data (Sequence Type data) to perform crossvalidation. This is a way to perform crossvalidation taking into account that some isolates might be highly related to others and therfore in order to keep our model as generalizable as possible, the training data is trained in a set of isolates belonging to  a particular ST but the validation testing is performed with isolates of a different set of ST groups. This technique has been applied for example in this [paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7059009/).\n",
        "\n",
        "Below we present the code to perform **blocked crossvalidation**, using the **MLST** column as the blocking factor. Others population structure factors used for blocking have been phylogenetical groups or SNP cutoff values, etc. The code is provided below for you to have the option to perform it, if you call the LG_hp_tune() function with cv = \"blocked\".\n",
        "\n"
      ],
      "metadata": {
        "id": "pw_n7OIQwoaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating function to perform hyper parameter tuning of model\n",
        "@ignore_warnings(category=ConvergenceWarning)\n",
        "def LG_hp_tune(param, feat_train_df, lab_train, v=3, cv=4):\n",
        "  #creating logistic regression model\n",
        "  model = LogisticRegression(random_state = 42, max_iter=500, class_weight='balanced', n_jobs=2)\n",
        "  # performing the hyper parameter tuning using crossvalidation\n",
        "  scoring_dic = {'f1_macro':make_scorer(f1_score , average='macro')}\n",
        "  # isolating the features used for training\n",
        "  feat = feat_train_df.drop(columns=[\"MLST\"])\n",
        "  # tranforming labels from letters to 0 & 1\n",
        "  lab_train_t = lab_train.replace({'R': 0, 'S': 1})\n",
        "  # Fitting using grid search parameters depending on crossvalidation scheme used\n",
        "  if str(cv).isnumeric():\n",
        "    cv = KFold(cv)\n",
        "    gs = GridSearchCV(model, param, scoring=scoring_dic,cv=cv, refit='f1_macro', verbose=v, return_train_score=True)\n",
        "    gs.fit(feat, lab_train_t)\n",
        "  elif cv == \"blocked\":\n",
        "    groups= feat_train_df['MLST']\n",
        "    cv = StratifiedGroupKFold(n_splits=4)\n",
        "    gs = GridSearchCV(model, param, scoring=scoring_dic,cv=cv, refit='f1_macro', verbose=v, return_train_score=True)\n",
        "    gs.fit(feat, lab_train_t, groups=groups)\n",
        "  else:\n",
        "    print(\"Please provide a valid crossvalidation scheme `blocked` or an integer\")\n",
        "  print(gs.best_params_)\n",
        "  print(gs.best_score_)\n",
        "  return gs.best_estimator_"
      ],
      "metadata": {
        "id": "fV7gsnwnAAU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the function created (takes long time)\n",
        "LG_tuned = LG_hp_tune(hparam, CTZ_GY_train_feat, CTZ_Train_test_dic['labels_train'])"
      ],
      "metadata": {
        "id": "cVLRjlzIoSxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the function created used **f1 score** to search for the best C parameter. The f1 score is a widely used score to interpret results where there is an imbalance in classes, since there are more S (1295) than R (206) isolates for CTZ in our dataset. We used the f1_score macro average, which takes into account the f1 scores for each class (R & S) and averages them. f1 scores range from 0 to 1.\n",
        "\n",
        "- At the end of our crossvalidation it shows that our best C parameter is 0.1\n",
        "- The highest average f1 score macro obtained from both of the classes is 0.821\n",
        "\n",
        "We can now use the improved (hyper parameter tuned ) model to predict labels."
      ],
      "metadata": {
        "id": "0D4F5eaZTj-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making predictions with best model from hyperparameter tuning\n",
        "CTZ_GY_tuned_pred = predict(LG_tuned,CTZ_GY_test_df)\n",
        "\n",
        "# observe how many predictions were made for each category \"R\" and \"S\"\n",
        "print(\"Labels predicted: \", np.unique(CTZ_GY_tuned_pred, return_counts=True))"
      ],
      "metadata": {
        "id": "RjQ2FO9ub-po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we can compare the evaluation between the model before and after tuning to see if there are any differences:\n",
        "\n",
        "**BEFORE Hyperparameter Tuning**\n",
        "\n",
        "Default C = 1.0\n"
      ],
      "metadata": {
        "id": "XuIRJNsm4ewC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation for LG model before tuning\n",
        "Model_Report = evaluate(LG_CTZ_GY_model, CTZ_Train_test_dic['labels_test'], CTZ_GY_labels_pred, cf=False)"
      ],
      "metadata": {
        "id": "auGCY4pl43C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AFTER Hyperparameter Tuning**\n",
        "\n",
        "Tuned C = 0.1"
      ],
      "metadata": {
        "id": "ccr8QygZHZG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation for LG model AFTER tuning\n",
        "Tuned_Model_Report = evaluate(LG_tuned, CTZ_Train_test_dic['labels_test'], CTZ_GY_tuned_pred, cf=False)"
      ],
      "metadata": {
        "id": "yOqCxxOt5C1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above we see the difference between the regular vs the tuned model, we see that the scores do not defer dramatically, except that R recall is a bit better now. Take into account that we have only performed tuning on one hyperparameter, perhaps if we did tuning for more hyperparameters our results could have been even better, however for the purposes of this tutorial, we will leave it at that.\n"
      ],
      "metadata": {
        "id": "O4KMBQ_uHgS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **9) Use functions created and evaluate every drug in every feature combination!**\n",
        "\n",
        "In this part we will combine all the functions we have created in order to get the results for different drug and feature combinations. You can choose whether to use the functions created in step 8 (will take longer but be more thorough) or those created before, with no parameter tuning (will be faster but a little less thorough).  "
      ],
      "metadata": {
        "id": "N5lGr0hoDCip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **a) Lets recall the list of drugs we have available and the combination of features we are interested in**"
      ],
      "metadata": {
        "id": "yExXAdaWZL4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check all drugs\n",
        "print(drug_list)\n",
        "\n",
        "# let's see all feature combinations we are interested in\n",
        "print(combo_list)"
      ],
      "metadata": {
        "id": "51wJM2XUaxtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **b) Create a loop that will go through all our functions using the lists above**\n",
        "\n",
        "The code below will take fairly long to run because we will train and hyperparameter tune several logistic regression models using different feature combinations for each of the 12 antibiotic drugs. It will then be stored in a python dictionary and dataframe so it can be accessed later.\n",
        "\n",
        "**NOTE:** Python Dictionaries, are common data structures within the python language that are very efficient at storing and retrieving information. It consists in key-value pairs, in our case each key is: drug_combo and the value is paired with are the metrics results."
      ],
      "metadata": {
        "id": "cZWO1rYocRQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for fname in os.listdir(filepath):\n",
        "    if fname.endswith('LG_metrics_df.csv'):\n",
        "        print(\"A csv with stored results for Logistic Regression has already been created. Please check your Google Drive directory.\")\n",
        "        break\n",
        "else:\n",
        "  # Lets use all our functions this time and save our report into a single data structure\n",
        "  LG_model_metrics = {}\n",
        "\n",
        "  for drug in drug_list:\n",
        "    print(drug)\n",
        "    # splits each drug df into a dictionary with testing and training data\n",
        "    Test_Train_dic = Split_train_test_antibiotic(drug)\n",
        "    for combo in combo_list:\n",
        "      # Training each drug and combo features\n",
        "      labels_train = Test_Train_dic[\"labels_train\"]\n",
        "      # create corresponding feature_df for training\n",
        "      features_train = combo_feat(Test_Train_dic[\"features_train\"], drug, combo)\n",
        "      print(drug+\"_\"+combo)\n",
        "\n",
        "      # runs logistic regression model using the corresponding training feature_df\n",
        "      LG_combo_model = run_LG(features_train, labels_train, drug, combo)\n",
        "\n",
        "      #Optional: If you want to use the hyperparameter tuning step 8 use this line instead of the previous one:\n",
        "      #LG_combo_model = LG_hp_tune(hparam, features_train, labels_train, v=0)\n",
        "\n",
        "      # create corresponding feature_df for testing\n",
        "      features_test = combo_feat(Test_Train_dic[\"features_test\"], drug, combo)\n",
        "      # generate predictions based on the feature combination tested\n",
        "      labels_pred = predict(LG_combo_model, features_test)\n",
        "      # loading the actual labels\n",
        "      labels_test = Test_Train_dic[\"labels_test\"]\n",
        "      # Evaluating our model\n",
        "      report = evaluate(LG_combo_model, labels_test, labels_pred, cf=False, show_results=False)\n",
        "      # Saving the results into a dictionary\n",
        "      LG_model_metrics[drug+\"_\"+combo] = report\n",
        "      print(report)\n",
        "  # convert dictionary into a dataframe\n",
        "  LG_metrics = pd.DataFrame.from_dict(LG_model_metrics, orient='index',columns=[\"Accuracy\", \"R_recall\", \"S_recall\", \"R_precision\", \"S_precision\"]).reset_index()\n",
        "  LG_metrics = LG_metrics.rename(columns = {'index':'Drug_combo'})\n",
        "\n",
        "  # saving our metric results into a CSV file\n",
        "  LG_metrics.to_csv(filepath+\"LG_metrics_df.csv\", index= False)\n"
      ],
      "metadata": {
        "id": "glSAeIj2DrnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **10) Create a bar graph showing metrics for all drugs using GY combination of features**\n",
        "\n",
        "Using the CSV we created above with all our results, we will create a bargraph to visualize metrics for every antibiotic that uses the feature combination GY (Gene Presence and Absence & Year of Isolate collection). In the graph below we are comparing Accuracy and Recall scores for both classes.\n",
        "\n",
        "The code below serves to create a new directory to store all our Machine Learning plots and figures created. If you run the code more than once it will let you know that the figure has already been created in the directory."
      ],
      "metadata": {
        "id": "2-6NzvrsbZaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # makes a directory for all your plot images\n",
        "  os.mkdir('/content/drive/My Drive/EColi_ML_Plots')\n",
        "except:\n",
        "  print(\"A directory was already created to store your plot\")\n",
        "\n",
        "# filtering for all the rows that contain GY features\n",
        "LG_metrics = pd.read_csv(\"/content/drive/MyDrive/EColi_ML_CSV_files/LG_metrics_df.csv\")\n",
        "GY_filter = [drug_combo for drug_combo in LG_metrics['Drug_combo'] if drug_combo.endswith(\"GY\")]\n",
        "GY_df = LG_metrics.loc[LG_metrics[\"Drug_combo\"].isin(GY_filter)]\n",
        "\n",
        "# Figure Size\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "fig = plt.figure(figsize =(15, 6))\n",
        "\n",
        "# Adding title\n",
        "plt.title('Accuracy, R_recall, S_recall', fontsize = 12)\n",
        "\n",
        "# Variables to be plotted\n",
        "x = np.arange(len(GY_df[\"Drug_combo\"]))\n",
        "acc = list(GY_df[\"Accuracy\"])\n",
        "R_rec = list(GY_df[\"R_recall\"])\n",
        "S_rec = list(GY_df[\"S_recall\"])\n",
        "\n",
        "# Plotting barcharts\n",
        "acc_bar=plt.bar(x-0.25, height= acc, width=0.25, color=\"grey\", edgecolor=\"gray\")\n",
        "rrec_bar=plt.bar(x, height= R_rec, width=0.25, color=\"plum\", edgecolor=\"gray\")\n",
        "srec_bar=plt.bar(x+0.25, height= S_rec, width=0.25, color=\"lavenderblush\", edgecolor=\"gray\")\n",
        "\n",
        "plt.xticks([r for r in range(len(GY_df[\"Drug_combo\"]))],\n",
        "            GY_df[\"Drug_combo\"], fontsize = 12)\n",
        "\n",
        "#legend\n",
        "fig.legend([acc_bar,rrec_bar,srec_bar],[\"Accuracy\", \"R_recall\", \"S_recall\"], bbox_to_anchor=(0.4,-0.35, 0.04, 0.4), fontsize=12)\n",
        "\n",
        "# Saving bargraph into our new directory\n",
        "plt.savefig('/content/drive/My Drive/EColi_ML_Plots/LG_GY_Accuracy_and_Recall_Scores.jpg',dpi=400, bbox_inches=\"tight\")\n",
        "\n",
        "# Show Plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dQs9pOMYvUg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice for instance the antibiotic **CTZ**, when we only pay attention to the Accuracy, we would conclude that our model is doing fairly well, however, when examine recall we can clearly see that the Recall for Resistance is noticeably lower than for Susceptibility. Therefore, for imbalanced classes, Accuracy is usually not the best metric to use.\n",
        "\n",
        "\n",
        "***OPEN QUESTION:***\n",
        "\n",
        "Look at the graph. Is recall typically better for R or S? why do you think that’s the case?"
      ],
      "metadata": {
        "id": "PXsvRlZk4fxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Thanks for making it this far! We have accomplished the objectives of this notebook. We learned the basics of how logistic regression works and learned how to make functions in order to run this model using Moragadivand's *E. coli* dataset. Now we will move on to our next ML model, [Random Forest](https://colab.research.google.com/drive/1a-DVckHvYOMehMZJWnX4Vu8m6t7ElXQy?usp=sharing) and learn a bit more about tree-based models in general."
      ],
      "metadata": {
        "id": "xngjGG3BF47x"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lucy-Moctezuma/ML-Tutorial-for-Antibiotic-Resistance-Predictions-for-E.-Coli/blob/main/4)_Extreme_Gradient_Boosted_Tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(**Note:**\n",
        "Click on the button that reads *“Open in Colab”* to open this code in Google Colab. Once open in Google Colab, you can make a copy of the notebook in your personal drive and run the code by clicking a little triangle/arrow to the left of each code block.)"
      ],
      "metadata": {
        "id": "a6ew2HnF3jep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extreme Gradient Boosted Tree**\n",
        "\n",
        "## ***Objectives for this Notebook***\n",
        "- Expand concepts learned about tree methods (ensemble methods) into Extreme Gradient Boosted Trees Algorithm.\n",
        "- Create functions to implement Gradient Boosted Trees into Moragadivand's dataset.\n",
        "\n",
        "**Extreme Gradient Boosted Tree (XGBoost)** is another ensemble method similar to Random Forests as it also uses several Decision Trees, each tree using a random subset of features and observations. XGBoost can use regressor trees or classifier trees as well. But we will only focus on using it for Classification. Below we have an outline of the main differences between Random Forests and XGBoost Trees.\n",
        "\n",
        "|<font size=4>Random Forest|<font size=4>XGBoost Tree|\n",
        "|:---|:---|\n",
        "|<font size=3>All trees are created as a group and are independent|<font size=3>Trees are created sequentially (one tree at a time) <br> and depend on the previous trees residual outputs|\n",
        "|<font size=3>Uses **bagging**: *Boostrapping* + *Aggregation* <br> - *Boostrapping:* random sampling with replacement<br> - *Aggregation:* adding all trees outputs and majority <br> votes decides final forecast|<font size=3>Uses **boosting** each subsequent tree attempts to <br> create a better predictor than the preceding one, <br>random sampling of observations happen without <br>replacement|\n",
        "|<font size=3>Individual trees predict target labels|<font size=3>Individual trees predict **residuals**, which is the difference  <br> between the predicted and observed prediction values|\n",
        "\n",
        "We will later detail how residuals are calculated as we get further into the notebook.\n",
        "\n",
        "## **XGBoost General Structure**\n",
        "\n",
        "![xgboost.jpg](https://drive.google.com/uc?export=view&id=12nVLJtdBGBJUp2EMFcoXPBg8c24ewFSg)\n",
        "\n",
        "Each classifier tree in the XGBoost Model is built differently than the ones for Random Forest, we will see in more detail the different parts of an XGBoost tree later on."
      ],
      "metadata": {
        "id": "4s_KUXfGSthq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1) Importing Packages needed**\n",
        "\n",
        "Something to note if you decide to check the source documentation is that the import for XGBoost Classifier model uses a different package. Unlike all our previous models, that use \"sklearn\", this model comes in a separate package named \"xgboost\".\n",
        "\n",
        "**NOTE:** Similar code as in previous notebook"
      ],
      "metadata": {
        "id": "fnpxHzYUUJ_M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDa820s0JSoJ"
      },
      "outputs": [],
      "source": [
        "# Data manipulation imports for ML\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas.api.types import is_numeric_dtype\n",
        "\n",
        "# Import packages for Gradient-Boosted Tree model and hyperparameter tunning\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.model_selection import StratifiedGroupKFold, KFold\n",
        "\n",
        "# Imports for model evaluation\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, get_scorer_names\n",
        "from sklearn.metrics import f1_score, make_scorer, accuracy_score, recall_score, precision_score\n",
        "import warnings\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "warnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)\n",
        "\n",
        "# Imports for data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from xgboost import plot_tree\n",
        "from xgboost import plot_importance\n",
        "\n",
        "# Imports for file management\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2) Loading CSV file and creating dataframes for each antibiotic**\n",
        "\n",
        "Similar to the previous notebook we loaded the dataframe created in the first notebook of this tutorial, then we create a dataframe for each antibiotic. To check that our function works we will test it using the \"AMP\" antibiotic.\n",
        "\n",
        "**NOTE:** Similar code as in previous notebook"
      ],
      "metadata": {
        "id": "1MTk8vozVJqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads csv file as a dataframe\n",
        "filepath = '/content/drive/My Drive/EColi_ML_CSV_files/'\n",
        "\n",
        "# reads csv file as a dataframe\n",
        "All_Drugs_df = pd.read_csv(filepath+\"EColi_Merged_df.csv\", na_values=\"NaN\")\n",
        "All_Drugs_df.head()"
      ],
      "metadata": {
        "id": "psqfFp_yVsmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3) Separating each Drug Dataframe into 4 sections : Training (features and labels) and Testing (features and labels)**\n",
        "\n",
        "**NOTE:** Similar code as in previous notebook"
      ],
      "metadata": {
        "id": "NMKzeaqPojso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating each dataframe into Labels and Features for training and testing data.\n",
        "# Our function uses the handy train_test_split() function.\n",
        "\n",
        "def Split_train_test(drug):\n",
        "  #here we make a list of the columns we want to keep: the column for the isolate, the column for the drug we are interested in and all features (starting from column 13).\n",
        "  df_list = [All_Drugs_df[[\"MLST\",\"Isolate\",drug,\"Year\"]], All_Drugs_df.iloc[:,15:]]\n",
        "\n",
        "  #here we create a data frame with just the columns we wanted to keep.\n",
        "  Drug_df = pd.concat(df_list, axis=1)\n",
        "\n",
        "  #here we drop all rows with missing data\n",
        "  Drug_df = Drug_df.dropna()\n",
        "\n",
        "  # Creating a dictionary to store each antibiotic datasets\n",
        "  Train_test_dic = {}\n",
        "\n",
        "  # Defining the label columns\n",
        "  labels = Drug_df[drug]\n",
        "\n",
        "  # Defining features columns\n",
        "  features = Drug_df.drop(columns=[drug])\n",
        "\n",
        "  # Separating training (features and labels) and testing (features and labels) datasets\n",
        "  features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.20, random_state=42, stratify=labels)\n",
        "\n",
        "  # storing each data chunk in a dictionary\n",
        "  Train_test_dic['labels_train'] = labels_train\n",
        "  Train_test_dic['features_train'] = features_train\n",
        "  Train_test_dic['labels_test'] = labels_test\n",
        "  Train_test_dic['features_test'] = features_test\n",
        "\n",
        "  return Train_test_dic"
      ],
      "metadata": {
        "id": "iP4HEW4qBO7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the function Split_train_test() for AMP example\n",
        "AMP_Train_test_dic = Split_train_test(\"AMP\")\n",
        "\n",
        "# checking the shape of each dataframe or series stored in the dictionary created for drug AMP\n",
        "print(\"AMP\")\n",
        "for k, df in AMP_Train_test_dic.items():\n",
        "  print(k, df.shape)\n",
        "  # counting how many of the labels have susceptible versus resistant ones\n",
        "  if k.startswith(\"labels\"):\n",
        "    print(df.value_counts())"
      ],
      "metadata": {
        "id": "mIFeC9JvsOgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time we have more Resistant E.Coli samples than Susceptible ones, unlike in our previous notebook. We will be looking at the AMP antibiotic and **the total number of training observations is 626.**"
      ],
      "metadata": {
        "id": "QjGPkIVPXx95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4) Creating different combination of features before training**\n",
        "\n",
        "**NOTE:** Same as prior notebook code"
      ],
      "metadata": {
        "id": "2IyqN506dQn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making a list of combinations of data sources we would like to test in our ML models\n",
        "combo_list = ['Y', 'G', 'GY']\n",
        "\n",
        "# making a function that creates different feature combinations of the predictor features\n",
        "def combo_feat(features_df, drug, combo):\n",
        "\n",
        "  # Isolating Year as a feature and\n",
        "  year_filter = [col for col in features_df if col.startswith(\"Year\")]\n",
        "  year_feat = features_df[year_filter]\n",
        "\n",
        "  # creating Gene precence column filters for features_df\n",
        "  gene_presc_filter = [col for col in features_df.columns if col not in year_filter and col != \"Isolate\"]\n",
        "  gene_presc_feat = features_df[gene_presc_filter]\n",
        "\n",
        "  if combo == 'Y':\n",
        "    df_list = [features_df[['MLST','Isolate']], year_feat]\n",
        "    Y_feat_df = pd.concat(df_list, axis=1)\n",
        "    Y_feat_df = Y_feat_df.drop(columns=['Isolate'])\n",
        "    return Y_feat_df\n",
        "\n",
        "  if combo == 'G':\n",
        "    df_list = [features_df['Isolate'], gene_presc_feat]\n",
        "    G_feat_df = pd.concat(df_list, axis=1)\n",
        "    G_feat_df = G_feat_df.drop(columns=['Isolate'])\n",
        "    return G_feat_df\n",
        "\n",
        "  if combo == 'GY':\n",
        "    df_list = [features_df['Isolate'], year_feat, gene_presc_feat]\n",
        "    GY_feat_df = pd.concat(df_list, axis=1)\n",
        "    GY_feat_df = GY_feat_df.drop(columns=['Isolate'])\n",
        "    return GY_feat_df"
      ],
      "metadata": {
        "id": "JmzbwQBxlylk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing combo_feat() function created for training data\n",
        "AMP_GY_train_df = combo_feat(AMP_Train_test_dic['features_train'],\"AMP\",\"GY\")\n",
        "\n",
        "# looking only at the feature column names for the combination for \"GY\" for drug \"AMP\" for training data\n",
        "AMP_GY_train_df.columns"
      ],
      "metadata": {
        "id": "iHsrRKadsRVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5) Creating Gradient Boosted Trees model and training it per feature combination**\n",
        "\n",
        "Notice that in the creation of an Extreme Gradient Boosted Classifier model within our function, the training labels need to be encoded, that is be transformed from **R** to **0** and **S** to **1**. Our function, will also print the number of trees created."
      ],
      "metadata": {
        "id": "ZVOKCEZNeHzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating Gradient-Boosted Trees model function\n",
        "def run_GB(feat_train_df, lab_train, drug, combo):\n",
        "  # creating encoder to transform R and S into 0 and 1 respectively\n",
        "  labels = lab_train\n",
        "  le = preprocessing.LabelEncoder()\n",
        "  le.fit(labels)\n",
        "  labels_t = le.transform(labels)\n",
        "  # Creating the XGB Classifier model\n",
        "  print(drug +\" Training combo: \"+ combo)\n",
        "  GB =  XGBClassifier(random_state = 42)\n",
        "  # Training XGB Classifier model\n",
        "  feat = feat_train_df.drop(columns=[\"MLST\"])\n",
        "  GB = GB.fit(feat, labels_t)\n",
        "  # Checking number of trees in the model\n",
        "  print(\"Number of Decicion Trees in XGB Classifier model:\", GB.n_estimators)\n",
        "  return GB"
      ],
      "metadata": {
        "id": "QbEP2WYkWPxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implementing run_GB() for specific drug feature combination dataframe\n",
        "GB_AMP_GY_model = run_GB(AMP_GY_train_df, AMP_Train_test_dic['labels_train'],\"AMP\",\"GY\")\n",
        "GB_AMP_GY_model"
      ],
      "metadata": {
        "id": "9H7HmGrWsUXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see there are many parameters we can tune in our XGB classifier. XGBoost is a preferred model by many practitioners because the model considers tuning how the decisions tree are created and trained (e.g. learning_rate and max_leaves). In addition, the model considers computer performance optimization (e.g. n_jobs) when calculating trees."
      ],
      "metadata": {
        "id": "yGblscmxbnOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **XGBoost Classification Tree**\n",
        "\n",
        "Unlike Random Forests, we notice that the XGBoost is made of smaller looking trees, in this case we have a tree with a 6 level depth. You can change and look at the other trees by changing the argument **num_trees**. Also note that one of the main differences is that these trees are not predicting labels, instead they predict residuals. In the picture below, we can focus on just the bottom part of one of the XGBoost Classifier trees and we will pretend that there are 4 residuals in the last intermediate node:\n",
        "\n",
        "Let's first take a look at the first XGBoost Tree created:"
      ],
      "metadata": {
        "id": "hXUdlj_cdLwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the first XGBoost Tree\n",
        "plot_tree(GB_AMP_GY_model, num_trees=0)\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(40, 15)"
      ],
      "metadata": {
        "id": "ZnkLhqlMgzgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Hypothetical Branch of a XGBoost Tree**\n",
        "\n",
        "Notice that unlike the Random Forest classifier trees, XGboost trees have by default only 6 levels (counting from below the root node). But To get a better idea of the differences between xgboost trees and the ones from our prior module, we can focus on just one of the sections output below, the one that starts with the Decision threshold <font face=\"monospace\">urf2 < 1</font>\n",
        "\n",
        "![XGBoost-branch.jpg](https://drive.google.com/uc?export=view&id=14V84Jbi1Awe-VnhWyo0nbwWdccc-s8GQ)\n",
        "***\n",
        "**Feature Threshold** (Example: <font face=\"monospace\">urf2 < 1</font>)\n",
        "\n",
        " This is the same threshold explained in our previous Random Forest Notebook, if the boolean statement is TRUE or \"Yes\" then it goes to the left leaf, if FALSE or \"No, missing\" then it goes to the right leaf. Meaning observations where this statement is FALSE or are missing would fall into the right branch.\n",
        "***\n",
        "**Residuals** (Example: <font face=\"monospace\">[r1,r5,r3,r7]</font>)\n",
        "\n",
        "Eventhough these are not displayed in the original graphed tree from the code, we can imagine how these residuals are classified so we have an idea of how the algorithm works. For the sake of simplifying the explanation, let's imagine that the node with Decision threshold <font face=\"monospace\">urf2 < 1</font> has 4 residuals, r1 is the residual for observation 1, r5 for observation 5, so on and so forth. In each of the nodes of an xgboost tree, the residuals for each observation are being classified as we descend on the tree, we have r1 and r5 in the left leaf and r3 and r7 in the right leaf.\n",
        "***\n",
        "**Leaf** (Example: <font face=\"monospace\">Leaf=0.338359058</font>)\n",
        "\n",
        "For XGboost trees, the leaves do not output the number of observations in each class, instead it displays a decimal, which is known as the **Output value**. This output is computed using only the residuals from the final leaf nodes, which in our pseudoexample would be r1 and r5.  Below its the actual formula to calculate it:\n",
        "\n",
        "$$Output =\\frac{\\sum Residuals}{\\sum[(Previous \\ Probability)*(1-Previous \\ Probability)] + \\lambda}$$\n",
        "\n",
        "- *$Residuals$* = The diference between the observed (0 for Resistance and 1 for Susceptibility) and predicted probabilities.\n",
        "\n",
        "- *$Previous \\ Probability$* = These are the probabilities predicted using the prior tree. Since this is an example from the first tree created, the default value for it is 0.5. This number will change as we keep building other trees.\n",
        "\n",
        "- *$\\lambda$*= Is the regularization term, the higher the number the lower the output value. This helps us to not follow too closely any particular observation so that our model generalizes well to new data (bias-variance trade-off). The default is set to 1. There are other regularization coeficcients, in this example *$\\lambda$* is equivalent to L2 regularization or Rigde Regression. Here is a [link](https://albertum.medium.com/l1-l2-regularization-in-xgboost-regression-7b2db08a59e0#:~:text=Therefore%2C%20the%20L1%20regularization%20decreases,lambda%20term%20increases%20the%20denominator.) for further mathematical explanation if interested.\n",
        "***\n",
        "\n",
        "Behind the simple looking XGboost tree there are different metrics used to decide each of the nodes and branches that shape the classifier tree. You can take a peak under the hood of this XGboost tree with the code below and check all the **Gain** and **Cover** scores at each split from the tree we graphed. We will briefly go over these concepts:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nH_v6rzWo-jI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# looking at the metrics for each node of the XGboost classifier Trees\n",
        "print(GB_AMP_GY_model.get_booster().get_dump(with_stats = True)[0])"
      ],
      "metadata": {
        "id": "pwXpvbJBvOuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **OUTPUT INTERPRETATION of get_dump() for one xgboost Tree**\n",
        "To interpret each **(root and intermediate)** nodes output, we can take look at the following structure:\n",
        "\n",
        "<font color=\"gray\">`Parent_node #:[feature threshhold] yes=left_child_#, no=right_child_#, missing=left_child_#, gain=gain, cover=cover`\n",
        "\n",
        "To interpret each **leaf** nodes output, we can examine the below structure:\n",
        "\n",
        "<font color=\"gray\">`leaf_node_#:leaf=output_value, cover=cover`\n",
        "***\n",
        "For each node in our tree, we calculate the following:\n",
        "\n",
        "- **Similarity Scores (SS):** The scores indicate how similar are a particular grouping of residuals. This score is important because similar residuals imply similarity in observations, which helps with their classification. Below we can see the formula for SS:\n",
        "\n",
        "$$\\frac{(\\sum Residuals)^2}{\\sum[(Previous \\ Probability)*(1-Previous \\ Probability)] + \\lambda}$$\n",
        "\n",
        "<a name=\"gain-def\"></a>\n",
        "- **Gain:** This is calculated for each of the intermediate nodes and measures the relative contribution of the corresponding feature to the model. The higher this value the better the feature is at making a prediction. Below is the formula:\n",
        "\n",
        "$$Gain = SS_{(Left)} + SS_{(Right)} - SS_{(Parent \\ Node)}$$\n",
        "\n",
        "- **Cover:** is a metric to measure the number of observations affected by the split. This is essentially if the first part of the denominator in the similarity score:\n",
        "\n",
        "$$Cover = \\sum[(Previous \\ Probability)*(1-Previous \\ Probability)]$$\n",
        "***\n",
        "**NOTE:** When researching this model it's important to know that XGBoost $\\neq$ Gradient Boosted Tree. The Reason it has the **Extreme** denomination is that XGBoost builds on Gradient Boosted Tree in that it add different methods of Regularization (to help with generalization of the model and combat overfitting) and parallel processing (increase computing efficiency).\n"
      ],
      "metadata": {
        "id": "s02Uuck7GIW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6) Making predictions from XGBoost model**\n",
        "\n",
        "**XGBoost Final Decision Made**\n",
        "\n",
        "To understand how the final decision is made by the entire XGBoost model, we need to revisit again the equations we learned on the very first ML model we used **Logistic Regression**. Because XGBoost Classifier models uses them as well!\n",
        "\n",
        "**Logit Equation:** converts probabilities into log-odds predictions.\n",
        "$$ ln(\\frac{P}{1-P}) = \\hat \\beta_0 + \\hat \\beta_jX$$\n",
        "\n",
        "**Logistic Regression Equation:** converts log-odds predictions into probability predictions.\n",
        "$$P = \\frac{e^{\\hat{\\beta}_{0}+\\hat{\\beta}_{j}X}}{1+e^{\\hat{\\beta}_{0}+\\hat{\\beta}_{j}X}}$$\n",
        "\n",
        "Turns out these Equations are the ones that enables the XGboost Classifier to convert the predictions made by each of these XGboost Classifier trees into probabilities that will ultimately help us classify our observations into **Resistant (R)** or **Susceptible (S)**. Below we can see a graph of how it works:\n",
        "\n",
        "![final_pred.jpg](https://drive.google.com/uc?export=view&id=1RbLJnuLNpKsNMCF2gv9YLWZYYaXcmPwQ)\n",
        "\n",
        "Iteration 1:\n",
        "**Predicted log (odds1) = Initial Log(odds) + LR * Leaf Output that observation falls into tree 1**\n",
        "\n",
        "Iteration 2:\n",
        "**Predicted log (odds2) = Predicted Log(odds1) + LR * Leaf Output that observation falls into tree 2**\n",
        "\n",
        "ETC...\n",
        "\n",
        "At the end after we iterate to all xgboost trees (in this case 100 times), at each iteration a new tree is added and each time the predicted log (odds) gets updated with a better aproximation until we get a final **Predicted log (odds100)**, which takes in consideration a total of 100 trees. This final prediction is converted to a probability using the logit and logistic regression equations. This should give us the final prediction of **Resistant (R) if probability < 0.5** or **Susceptible (S) if probability > 0.5**\n",
        "\n",
        "**NOTE:** The Learning Rate in XGBoost is also called **\"eta\"** in other sources.\n",
        "***\n",
        "\n",
        "Below we can run a short piece of code to check the improvements for every 10 iterations out of a total of 100 to test how an XGBoost model works, we are not actually using our own model, the code creates automatically a training and validation sets, from the training data:"
      ],
      "metadata": {
        "id": "1px3jn-Djn77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creates a copy of the labels\n",
        "num_labels = AMP_Train_test_dic['labels_train'].copy()\n",
        "\n",
        "# transforms all labels of S into 1 and all labels of R into 0\n",
        "num_labels.loc[num_labels == \"S\"] = 1.0\n",
        "num_labels.loc[num_labels == \"R\"] = 0.0\n",
        "\n",
        "# create a matrix using the only the training data and the training labels coded as numbers\n",
        "feat_df = AMP_GY_train_df.drop(columns=[\"MLST\"])\n",
        "d = xgb.DMatrix(feat_df, label=num_labels)\n",
        "\n",
        "# set up basic parameters that are similar to our model\n",
        "p = {'max_depth':6, 'eta':0.33, 'objective':'binary:logistic'}\n",
        "\n",
        "# prints results for every 10 iterations out of 100\n",
        "res = xgb.cv(params = p, dtrain=d, num_boost_round=100, verbose_eval=10, metrics=\"error\")"
      ],
      "metadata": {
        "id": "UPGNb3tTWb8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above shows us that the overall trend is that the error rate decreases as number of iteration increases. The outputs are composed of 2 numbers in the following format:\n",
        "\n",
        "`train-error: #1 + #2     test-error: #1 + #2`\n",
        "\n",
        "- **#1** is the error shown for training and testing sets, it is essentially (1- accuracy) for every 10th iteration.\n",
        "- **#2** is the standard deviation of the error rates.\n",
        "\n",
        "\n",
        "Now that we have an idea of how the XGBoost tree trains and makes its final predictions using all the trees. We can now make predictions with it, below a simple function is created similar to the code in the previous notebook:"
      ],
      "metadata": {
        "id": "EZCKtwd9kkkW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzEGZrwHAZ3W"
      },
      "outputs": [],
      "source": [
        "def predict(GB_combo_Model, features_test):\n",
        "  feat = features_test.drop(columns=['MLST'])\n",
        "  labels_pred = GB_combo_Model.predict(feat)\n",
        "  if is_numeric_dtype(labels_pred):\n",
        "    # tranforming labels from numbers to letters\n",
        "    lab_pred_t = labels_pred.astype('O')\n",
        "    lab_pred_t[lab_pred_t==0] = 'R'\n",
        "    lab_pred_t[lab_pred_t==1] = 'S'\n",
        "    return lab_pred_t\n",
        "  else:\n",
        "    return labels_pred"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing combo_feat() function created for testing data\n",
        "AMP_GY_test_df = combo_feat(AMP_Train_test_dic['features_test'],\"AMP\",\"GY\")\n",
        "\n",
        "# looking only at the feature column names for the combination for \"GY\" for drug \"AMP\" for testing data\n",
        "AMP_GY_test_df.shape"
      ],
      "metadata": {
        "id": "3Nj98S-BsblR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of the predict() function using the feature combination \"GS\"\n",
        "AMP_GY_labels_pred = predict(GB_AMP_GY_model,AMP_GY_test_df)\n",
        "\n",
        "# observe how many predictions were made for each category \"R\" and \"S\"\n",
        "print(\"Labels predicted: \", np.unique(AMP_GY_labels_pred, return_counts=True))"
      ],
      "metadata": {
        "id": "jquWRTCqsX50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our **XGBoost model** has predicted 118 Resistant observations and 39 Susceptible observations."
      ],
      "metadata": {
        "id": "2ZvKLErkIEP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7) Evaluating our model using a confusion matrix and metrics**\n",
        "\n",
        "Similarly to the previous notebook, we will evaluate our XGBoost model by using a Confusion Matrix and respective metrics. Below is a quick review of these, remember that there is one Accuracy score, but Recall and Presicion should have as many sets as classes our model its trained to predict:\n",
        "\n",
        "|<font size=3>Metrics|<font size=3>General formula| <font size=3>Formula for 2 classes|\n",
        "|--|:-:|:-:|\n",
        "|<font size=3>**Accuracy**|<font size=3>$\\frac{Correctly \\ classified}{All \\ Predicted}$|<font size=3>$\\frac{TP + TN}{TP + TN + FN + FP}$|\n",
        "|<font size=3>**R Recall:**|<font size=3>$\\frac{Correctly \\ classified \\ as \\ R}{All \\ Actual \\ R}$|<font size=3>$\\frac{TP}{TP + FN}$|\n",
        "|<font size=3>**R Precision:**|<font size=3>$\\frac{Correctly \\ classified \\ as \\ R}{All \\ Predicted \\ R}$|<font size=3>$\\frac{TP}{TP + FP}$|\n",
        "\n",
        "**NOTE:** Code below is the same as in previous notebook\n"
      ],
      "metadata": {
        "id": "5dox3JfFn5wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function that evaluates our model using our actual and predicted data\n",
        "def evaluate(GB_combo_model, labels_test, labels_pred, cf= True, show_results=True):\n",
        "  report = classification_report(labels_test, labels_pred, output_dict = True)\n",
        "  if cf == True:\n",
        "    cm = confusion_matrix(labels_test, labels_pred, labels=np.where(GB_combo_model.classes_<1,\"R\",\"S\"))\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.where(GB_combo_model.classes_<1,\"R\",\"S\"))\n",
        "    disp.plot()\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "  if show_results == True:\n",
        "    print(\"Results\")\n",
        "    print('Accuracy:',report['accuracy'])\n",
        "    print('R recall:',report['R']['recall'])\n",
        "    print('S recall:',report['S']['recall'])\n",
        "    print('R precision:',report['R']['precision'])\n",
        "    print('S precision:',report['S']['precision'])\n",
        "  return [report['accuracy'], report['R']['recall'], report['S']['recall'], report['R']['precision'], report['S']['precision']]"
      ],
      "metadata": {
        "id": "YJJmoOq-o97t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implementing the evaluate() function\n",
        "Model_Report = evaluate(GB_AMP_GY_model, AMP_Train_test_dic['labels_test'],AMP_GY_labels_pred)"
      ],
      "metadata": {
        "id": "UGJIGTcNsgeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b) Create a feature importance graph showing which features were the most important**\n",
        "\n",
        "There are actually different ways to evaluate the importance of features in this model, some are based on the **Cover**, others in **Gain**, etc. Since we are already familiar with how these are calculated in [Part 5](#gain-def).\n",
        "\n",
        "The plot below will focus on the **Gain**. Since every node in every tree of our XGBoost model have their own gain, the value shown is just the average of each node. The Gain is the most relevant attribute to interpret the relative importance of each feature."
      ],
      "metadata": {
        "id": "os7345fZRPJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting feature importance (\"gain\")\n",
        "feat_imp = GB_AMP_GY_model.get_booster().get_score(importance_type=\"gain\")\n",
        "for k in feat_imp.keys():\n",
        "    feat_imp[k] = round(feat_imp[k],2)\n",
        "\n",
        "plot_importance(feat_imp, max_num_features = 20, importance_type = \"gain\", show_values=True, height=0.7)\n",
        "plt.savefig('/content/drive/My Drive/EColi_ML_Plots/GB_AMP_GY_feat_importance.jpg',dpi=400, bbox_inches=\"tight\")"
      ],
      "metadata": {
        "id": "H5Z-7_SlRwQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we can see that the most important feature in the model **GB_AMP_GY_model**, based in **Gain** metric is the gene `bla`.\n",
        "The Gain implies the relative contribution of the feature to the model. It takes each feature's contribution for each tree in the model. A higher value of this metric when compared to another feature implies it is more important for generating a prediction."
      ],
      "metadata": {
        "id": "uqjCgb-FdQjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8) (OPTIONAL) Hyperparameter Tunning and Crossvalidation for XGBoost model**\n",
        "\n",
        "This section of the notebook is optional.\n",
        "Here we will carry out hyperparameter tuning using cross-validation to optimize the model's performance and ensure its robustness. We will also include code to use a blocked cross-validation design. These approaches are important to make good ML models. However, if you are just here to learn the basics, feel free to skip section 8."
      ],
      "metadata": {
        "id": "PRDwJXOZKErj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **a) Hyperparameter Tunning**\n",
        "\n",
        "For now we will just perform the hyperparameter tunning for \"eta\" (seen in part 6) which ranges from 0 to 1, which is the learning rate for each xgboost tree.\n",
        "If you want to tune other hyperparameters in your model feel free to read the [documentation](https://xgboost.readthedocs.io/en/stable/parameter.html) for each of them.\n",
        "\n",
        "As usual we start by creating a dictionary with values to test:\n"
      ],
      "metadata": {
        "id": "ACfooEzAmuXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# performing hyperparameter tunning for model\n",
        "## Creating dictionary of parameters to tune.\n",
        "hparam = {\"eta\":[0.1, 0.2, 0.4, 0.8]}"
      ],
      "metadata": {
        "id": "gFxbVD-SLYEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **b) Crossvalidation**"
      ],
      "metadata": {
        "id": "UY3HNsonLwAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating function to perform hyper parameter tunning of model\n",
        "def GB_hp_tune(param, feat_train_df, lab_train, v=3, cv=4):\n",
        "  # Creating XGBOOST model\n",
        "  model = XGBClassifier(random_state = 42)\n",
        "  # performing the hyper parameter tunning using crossvalidation\n",
        "  scoring_dic = {'f1_macro':make_scorer(f1_score , average='macro')}\n",
        "  # isolating the features used for training\n",
        "  feat = feat_train_df.drop(columns=[\"MLST\"])\n",
        "  # tranforming labels from letters to 0 & 1\n",
        "  lab_train_t = lab_train.replace({'R': 0, 'S': 1})\n",
        "  # Fitting using grid search parameters depending on crossvalidation scheme used\n",
        "  if str(cv).isnumeric():\n",
        "    cv = KFold(cv)\n",
        "    gs = GridSearchCV(model, param, scoring=scoring_dic,cv=cv, refit='f1_macro', verbose=v, return_train_score=True)\n",
        "    gs.fit(feat, lab_train_t)\n",
        "  elif cv == \"blocked\":\n",
        "    groups= feat_train_df['MLST']\n",
        "    cv = StratifiedGroupKFold(n_splits=4)\n",
        "    gs = GridSearchCV(model, param, scoring=scoring_dic,cv=cv, refit='f1_macro', verbose=v, return_train_score=True)\n",
        "    gs.fit(feat, lab_train_t, groups=groups)\n",
        "  else:\n",
        "    print(\"Please provide a valid crossvalidation scheme `blocked` or an integer\")\n",
        "  print(gs.best_params_)\n",
        "  print(gs.best_score_)\n",
        "  return gs.best_estimator_"
      ],
      "metadata": {
        "id": "tVBB5kHzLzPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the function created (takes longer time)\n",
        "GB_tunned = GB_hp_tune(hparam, AMP_GY_train_df, AMP_Train_test_dic['labels_train'])"
      ],
      "metadata": {
        "id": "oi3Il8pUDqHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best xgboot model for AMP antibiotic using Year and Gene Presence/Absence features was obtained using eta = 0.2, which produced an f1_macro score of 0.9014"
      ],
      "metadata": {
        "id": "ogqRPFVhJgIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **9) Use all functions and evaluate every drug in every feature combination!**\n",
        "**NOTE:** Code below is similar as in previous notebook"
      ],
      "metadata": {
        "id": "N5lGr0hoDCip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **a) Lets recall the list of drugs we have available and the combination of features we are interested in**"
      ],
      "metadata": {
        "id": "yExXAdaWZL4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check all drugs\n",
        "drug_list = All_Drugs_df.iloc[:,3:15].columns\n",
        "print(drug_list)\n",
        "\n",
        "# let's see all combinations we are interested in\n",
        "print(combo_list)"
      ],
      "metadata": {
        "id": "51wJM2XUaxtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **b) Create a loop that will go through all our functions using the lists above**"
      ],
      "metadata": {
        "id": "cZWO1rYocRQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for fname in os.listdir(filepath):\n",
        "    if fname.endswith('GB_metrics_df.csv'):\n",
        "        print(\"A csv with stored results for XGBoost has already been created. Please check your Google Drive directory.\")\n",
        "        break\n",
        "else:\n",
        "  # Lets use all our functions this time and save our report into a single data structure\n",
        "  GB_model_metrics = {}\n",
        "\n",
        "  for drug in drug_list:\n",
        "    print(drug)\n",
        "    # splits each drug df into a dictionary with testing and training data\n",
        "    Test_Train_dic = Split_train_test(drug)\n",
        "    for combo in combo_list:\n",
        "      # Training each drug and combo features\n",
        "      labels_train = Test_Train_dic[\"labels_train\"]\n",
        "      # create corresponding feature_df for training\n",
        "      features_train = combo_feat(Test_Train_dic[\"features_train\"], drug, combo)\n",
        "      print(drug+\"_\"+combo)\n",
        "\n",
        "      # runs xgboost model using the corresponding training feature_df\n",
        "      GB_combo_model = run_GB(features_train, labels_train, drug, combo)\n",
        "\n",
        "      #Optional: If you want to use the hyperparamter tuning step 8 use this line instead of the previous one:\n",
        "      #GB_combo_model = GB_hp_tune(hparam, features_train, labels_train, v=0)\n",
        "\n",
        "      # create corresponding feature_df for testing\n",
        "      features_test = combo_feat(Test_Train_dic[\"features_test\"], drug, combo)\n",
        "      # generate predictions based on the feature combination tested\n",
        "      labels_pred = predict(GB_combo_model, features_test)\n",
        "      # loading the actual labels\n",
        "      labels_test = Test_Train_dic[\"labels_test\"]\n",
        "      # Evaluating our model\n",
        "      report = evaluate(GB_combo_model, labels_test, labels_pred, cf=False, show_results=False)\n",
        "      # Saving the results into a dictionary\n",
        "      GB_model_metrics[drug+\"_\"+combo] = report\n",
        "      print(report)\n",
        "  # convert dictionary into a dataframe\n",
        "  GB_metrics = pd.DataFrame.from_dict(GB_model_metrics, orient='index',columns=[\"Accuracy\", \"R_recall\", \"S_recall\", \"R_precision\", \"S_precision\"]).reset_index()\n",
        "  GB_metrics = GB_metrics.rename(columns = {'index':'Drug_combo'})\n",
        "\n",
        "  # saving our metric results into a CSV file\n",
        "  GB_metrics.to_csv(filepath+\"GB_metrics_df.csv\", index= False)"
      ],
      "metadata": {
        "id": "DhFxDZCrImop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **10) Create a bar graph showing metrics for all drugs using only GY features**\n",
        "\n",
        "Using the CSV we created above with all our results, we will create a bargraph to visualize metrics for every antibiotic that uses the GY feature (Gene Presence/Absence and Year). In the graph below we are comparing Accuracy and Precision scores for both classes.\n",
        "\n",
        "The code below serves to create a new directory to store all our Machine Learning plots and figures created. If you run the code more than once it will let you know that the figure has already been created in the directory."
      ],
      "metadata": {
        "id": "myTzObQnMBir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # makes a directory for all your plot images\n",
        "  os.mkdir('/content/drive/My Drive/EColi_ML_Plots')\n",
        "except:\n",
        "  print(\"A directory was already created to store your plot\")\n",
        "\n",
        "# filtering for all the rows that contain GY features\n",
        "GB_metrics = pd.read_csv(\"/content/drive/MyDrive/EColi_ML_CSV_files/GB_metrics_df.csv\")\n",
        "GY_filter = [drug_combo for drug_combo in GB_metrics['Drug_combo'] if drug_combo.endswith(\"GY\")]\n",
        "GY_df = GB_metrics.loc[GB_metrics[\"Drug_combo\"].isin(GY_filter)]\n",
        "\n",
        "# Figure Size\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "fig = plt.figure(figsize =(15, 6))\n",
        "\n",
        "# Adding title\n",
        "plt.title('Accuracy, R_precision, S_precision', fontsize = 12)\n",
        "\n",
        "# Variables to be plotted\n",
        "x = np.arange(len(GY_df[\"Drug_combo\"]))\n",
        "acc = list(GY_df[\"Accuracy\"])\n",
        "R_prec = list(GY_df[\"R_precision\"])\n",
        "S_prec = list(GY_df[\"S_precision\"])\n",
        "\n",
        "# Plotting barcharts\n",
        "acc_bar=plt.bar(x-0.25, height= acc, width=0.25, color=\"grey\", edgecolor=\"gray\")\n",
        "rprec_bar=plt.bar(x, height= R_prec, width=0.25, color=\"goldenrod\", edgecolor=\"gray\")\n",
        "sprec_bar=plt.bar(x+0.25, height= S_prec, width=0.25, color=\"khaki\", edgecolor=\"gray\")\n",
        "\n",
        "plt.xticks([r for r in range(len(GY_df[\"Drug_combo\"]))],\n",
        "            GY_df[\"Drug_combo\"], fontsize = 12)\n",
        "\n",
        "#legend\n",
        "fig.legend([acc_bar,rprec_bar,sprec_bar],[\"Accuracy\", \"R_precision\", \"S_precision\"], bbox_to_anchor=(0.4,-0.35, 0.04, 0.4), fontsize=12)\n",
        "\n",
        "# Saving bargraph into our new directory\n",
        "plt.savefig('/content/drive/My Drive/EColi_ML_Plots/GB_GY_Accuracy_and_Recall_Scores.jpg',dpi=400, bbox_inches=\"tight\")\n",
        "\n",
        "# Show Plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Zh8e4vNyMX5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have finally created and saved all the XGBoost Classifier results for every antibiotic and combination of features we were interested in. Next we will be moving away from tree-based or ensemble methods and learn about our last Machine Learning Method: [Neural Networks](https://colab.research.google.com/drive/1ytPswhUtQjNzRmJbZaFVw4_SGX5wG7iK?usp=drive_link)."
      ],
      "metadata": {
        "id": "KJVozj97pWvR"
      }
    }
  ]
}